{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"caa203f3bcae4899bd3d024a852f6ac1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9440cb2331a34171b2be94f43c8ab9b9","IPY_MODEL_735efa5b834b496c814a828f27931ac9","IPY_MODEL_c6c4e09286624d23b2df7ef165e1dfe9"],"layout":"IPY_MODEL_59ab2c64d20e483e83c7d60fb93fd3b3"}},"9440cb2331a34171b2be94f43c8ab9b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4de4e72949f346c186ce87b77f90389e","placeholder":"​","style":"IPY_MODEL_6a5b89f5460b4c36850cee601d3bd3dc","value":"Downloading: 100%"}},"735efa5b834b496c814a828f27931ac9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ee89fddb411423797cfb8c0814c0db4","max":1880,"min":0,"orientation":"horizontal","style":"IPY_MODEL_155d7d8378fc4ebda4d0ca151853d41b","value":1880}},"c6c4e09286624d23b2df7ef165e1dfe9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79c42b7e0dfb423cacd424709c64f3ac","placeholder":"​","style":"IPY_MODEL_8ecfa5386e9241bd9f183f3e1cafdc8c","value":" 1.88k/1.88k [00:00&lt;00:00, 81.3kB/s]"}},"59ab2c64d20e483e83c7d60fb93fd3b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4de4e72949f346c186ce87b77f90389e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a5b89f5460b4c36850cee601d3bd3dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ee89fddb411423797cfb8c0814c0db4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"155d7d8378fc4ebda4d0ca151853d41b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"79c42b7e0dfb423cacd424709c64f3ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ecfa5386e9241bd9f183f3e1cafdc8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e15114244431409b8600173d272577c2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_38c08768ec7f46adb789e7354d162aa4","IPY_MODEL_a4de88b476294e4ea4bad720ab6f06eb","IPY_MODEL_e6e75dac81684ef38649d42291161eb9"],"layout":"IPY_MODEL_26943267242f41e2ad55ab1e5ffb9385"}},"38c08768ec7f46adb789e7354d162aa4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82d9969e4a9f4ccd99d40fb09269c7aa","placeholder":"​","style":"IPY_MODEL_59723f568edd41b09792c74080741874","value":"Downloading: 100%"}},"a4de88b476294e4ea4bad720ab6f06eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_11c8bb2274e141e19f37b9b3ee8b0071","max":498723565,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a8da97f63b74e34b5bd86a32a5a448f","value":498723565}},"e6e75dac81684ef38649d42291161eb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_540a198a2c2e414984cc1465befcf630","placeholder":"​","style":"IPY_MODEL_39f2328855fe40138b39644b50ebb6b1","value":" 499M/499M [00:18&lt;00:00, 36.6MB/s]"}},"26943267242f41e2ad55ab1e5ffb9385":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82d9969e4a9f4ccd99d40fb09269c7aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59723f568edd41b09792c74080741874":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11c8bb2274e141e19f37b9b3ee8b0071":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a8da97f63b74e34b5bd86a32a5a448f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"540a198a2c2e414984cc1465befcf630":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39f2328855fe40138b39644b50ebb6b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35e4618d046449f4b70c85d636f2693e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73860c71d1fc47598e3ca438a110ebb0","IPY_MODEL_cd754e5f760948b9be1250906ad24658","IPY_MODEL_dce4470a9ad145f7a34536871de17985"],"layout":"IPY_MODEL_c76a2ca3023744b69dc5d9185eb41ce9"}},"73860c71d1fc47598e3ca438a110ebb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21bc5ebe9d064b7785b64b6622976d50","placeholder":"​","style":"IPY_MODEL_2be6e908487c491c8ae1fc0d4e90204a","value":"Downloading: 100%"}},"cd754e5f760948b9be1250906ad24658":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e16839495574769adb1a456079270ca","max":1302,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a7ca51cd3dd549e1b8fce4009fcecdf1","value":1302}},"dce4470a9ad145f7a34536871de17985":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e36457d267c43f4b2ad67e07a82cff9","placeholder":"​","style":"IPY_MODEL_b51b2fecfe8d4916b04b904aaea97365","value":" 1.30k/1.30k [00:00&lt;00:00, 30.9kB/s]"}},"c76a2ca3023744b69dc5d9185eb41ce9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21bc5ebe9d064b7785b64b6622976d50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2be6e908487c491c8ae1fc0d4e90204a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e16839495574769adb1a456079270ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7ca51cd3dd549e1b8fce4009fcecdf1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6e36457d267c43f4b2ad67e07a82cff9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b51b2fecfe8d4916b04b904aaea97365":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54d9ab04375d43098cd3b9e2e3afe0ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27e3865050074e1d91a9754a5128ffa3","IPY_MODEL_536490dd7295450cb23cadec6feda145","IPY_MODEL_1364771140ec4e11920afc714a855357"],"layout":"IPY_MODEL_67a422eaaf6a412481316a77e5a58985"}},"27e3865050074e1d91a9754a5128ffa3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9877bc50bbdb4e49b284615f8d3a4f8e","placeholder":"​","style":"IPY_MODEL_91d0056bc2f146c9a7df2087133264d2","value":"Downloading: 100%"}},"536490dd7295450cb23cadec6feda145":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd699c24a6c04e479b214ea104ec9ea2","max":798293,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5cf7e1d020d4dc0a90ad36ea6b07d93","value":798293}},"1364771140ec4e11920afc714a855357":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fc926db87e44e148dd54976d731041f","placeholder":"​","style":"IPY_MODEL_198f6a9edce040cf9d462f37b223e129","value":" 798k/798k [00:00&lt;00:00, 1.29MB/s]"}},"67a422eaaf6a412481316a77e5a58985":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9877bc50bbdb4e49b284615f8d3a4f8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91d0056bc2f146c9a7df2087133264d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd699c24a6c04e479b214ea104ec9ea2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5cf7e1d020d4dc0a90ad36ea6b07d93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6fc926db87e44e148dd54976d731041f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"198f6a9edce040cf9d462f37b223e129":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0b1b718982b414084a01e5919b84d47":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bb70c0eea1a7460faee612e73492b828","IPY_MODEL_a58e54aad81d4e78a003d3adbe621459","IPY_MODEL_01c10f7a94cc4b7691ac771078ae7063"],"layout":"IPY_MODEL_36b21a7e65a446a980e065d0ea2e134d"}},"bb70c0eea1a7460faee612e73492b828":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_320eb532e03b4c2f9b249d56d319e375","placeholder":"​","style":"IPY_MODEL_6f79fc07cee8420db3a0db91af47de0e","value":"Downloading: 100%"}},"a58e54aad81d4e78a003d3adbe621459":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cef64cf468794ac8bb09d12124289d9c","max":456356,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3bf887eb3e244aec894f70ce0ed0d683","value":456356}},"01c10f7a94cc4b7691ac771078ae7063":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dcfeee46fd74c6cb3eb64e2286ecf5d","placeholder":"​","style":"IPY_MODEL_65d660761a644381bdc0dbffa5067572","value":" 456k/456k [00:00&lt;00:00, 853kB/s]"}},"36b21a7e65a446a980e065d0ea2e134d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"320eb532e03b4c2f9b249d56d319e375":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f79fc07cee8420db3a0db91af47de0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cef64cf468794ac8bb09d12124289d9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bf887eb3e244aec894f70ce0ed0d683":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4dcfeee46fd74c6cb3eb64e2286ecf5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65d660761a644381bdc0dbffa5067572":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e79fc2ff46e7429db051fe3ba006374e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9aaefe0b6bdd4eff9da6eae027551013","IPY_MODEL_005189bd4a284bb69201b3e3b155d4cd","IPY_MODEL_c18ec8136a294248bce12eac1ba07270"],"layout":"IPY_MODEL_335aa29ca95e4553a5ae0fd508d5df78"}},"9aaefe0b6bdd4eff9da6eae027551013":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a7c301d963543a5851bf7bd414fe7b9","placeholder":"​","style":"IPY_MODEL_50f8f023a8854736920d579e3e75a648","value":"Downloading: 100%"}},"005189bd4a284bb69201b3e3b155d4cd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4029d40ec9264c61a8010324b25ecd7a","max":1355881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c809c84279054165910679503937cb14","value":1355881}},"c18ec8136a294248bce12eac1ba07270":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c217aa12caf34541aa8b0a97e7462f89","placeholder":"​","style":"IPY_MODEL_543a31160f9143bebe7d70b0bc7b7604","value":" 1.36M/1.36M [00:00&lt;00:00, 5.01MB/s]"}},"335aa29ca95e4553a5ae0fd508d5df78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a7c301d963543a5851bf7bd414fe7b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50f8f023a8854736920d579e3e75a648":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4029d40ec9264c61a8010324b25ecd7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c809c84279054165910679503937cb14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c217aa12caf34541aa8b0a97e7462f89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"543a31160f9143bebe7d70b0bc7b7604":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5778226b43634ad18c199e94a3209997":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2bda65e667c54469b3f6d2f9967768de","IPY_MODEL_8e464023d6dc48cc94d078755b6b1dfb","IPY_MODEL_5c4804f2a196426e920fa94e2adcff52"],"layout":"IPY_MODEL_9541a33af693407bb4ed5437b2e810c3"}},"2bda65e667c54469b3f6d2f9967768de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab4103e4efb74767bf3c3e3fd62e4fb5","placeholder":"​","style":"IPY_MODEL_02abca03c7144049a7e0872810e1c8e3","value":"Downloading: 100%"}},"8e464023d6dc48cc94d078755b6b1dfb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47abb008933b41879281b0f5c3e5f4a1","max":239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_288e4edb004e4965908b7b5c41dcdb47","value":239}},"5c4804f2a196426e920fa94e2adcff52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5212aeac7d1d46ea89f2a7f1848a3681","placeholder":"​","style":"IPY_MODEL_7368413b863240298be01a59cdb1f096","value":" 239/239 [00:00&lt;00:00, 5.79kB/s]"}},"9541a33af693407bb4ed5437b2e810c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab4103e4efb74767bf3c3e3fd62e4fb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02abca03c7144049a7e0872810e1c8e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47abb008933b41879281b0f5c3e5f4a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"288e4edb004e4965908b7b5c41dcdb47":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5212aeac7d1d46ea89f2a7f1848a3681":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7368413b863240298be01a59cdb1f096":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da923a7a2a0a4c48bdd5a30b3ab15b30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a334bd5e3445440f9b954801a3c8dfb6","IPY_MODEL_377b8753638c48abb9fdee08690e4781","IPY_MODEL_71c1a51e888e4715b7a91f1bc0c7393d"],"layout":"IPY_MODEL_fc4d89eb5df74e86adafed29c90d7d90"}},"a334bd5e3445440f9b954801a3c8dfb6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_680c76db9b934340b84e384ffb4697e6","placeholder":"​","style":"IPY_MODEL_2e75ff54c426405ba8242737766b0c1d","value":"Downloading: 100%"}},"377b8753638c48abb9fdee08690e4781":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6288eab646d4a289fbdb2443568572e","max":980,"min":0,"orientation":"horizontal","style":"IPY_MODEL_032ba18594f7447e9c0c23f7cc676fbf","value":980}},"71c1a51e888e4715b7a91f1bc0c7393d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86e686defb39455f958a3efcec02312c","placeholder":"​","style":"IPY_MODEL_6cb942a47f5c4d5ca4f947813b2ecb88","value":" 980/980 [00:00&lt;00:00, 22.5kB/s]"}},"fc4d89eb5df74e86adafed29c90d7d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"680c76db9b934340b84e384ffb4697e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e75ff54c426405ba8242737766b0c1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6288eab646d4a289fbdb2443568572e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"032ba18594f7447e9c0c23f7cc676fbf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86e686defb39455f958a3efcec02312c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cb942a47f5c4d5ca4f947813b2ecb88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8deefbdccf9a4187b42ce1f3e012beca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_197700a999bb45a5910de24352791fbb","IPY_MODEL_7e1395fff8c2411d8243706375a90cec","IPY_MODEL_ae99cbf3d81a4a1c931a4007c5430e59"],"layout":"IPY_MODEL_c07c397d05cc4ba5a2066181f985ffd1"}},"197700a999bb45a5910de24352791fbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a83c11a4b57541c8a9f1ec5a271d5e8e","placeholder":"​","style":"IPY_MODEL_1674adb99edc43b694f464025985104d","value":"Downloading: 100%"}},"7e1395fff8c2411d8243706375a90cec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f454091866d74c329905a4b8606376d4","max":539691437,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb689783b02949fb8b0646cddb207d28","value":539691437}},"ae99cbf3d81a4a1c931a4007c5430e59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c0534b99a6549d1aa09d48645fe53b0","placeholder":"​","style":"IPY_MODEL_bd1b419df1cc40328629fe6f9c233108","value":" 540M/540M [00:13&lt;00:00, 10.4MB/s]"}},"c07c397d05cc4ba5a2066181f985ffd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a83c11a4b57541c8a9f1ec5a271d5e8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1674adb99edc43b694f464025985104d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f454091866d74c329905a4b8606376d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb689783b02949fb8b0646cddb207d28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c0534b99a6549d1aa09d48645fe53b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd1b419df1cc40328629fe6f9c233108":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1c4a2d6a4d94726adce67baae1057e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd0192b494e24347b283f31d45f510cf","IPY_MODEL_ec966b82e9b948d99a512650c5a2872a","IPY_MODEL_fb2bf8adf273424cab5768005f5aa3b5"],"layout":"IPY_MODEL_10c334bf52664b08bc817503ed20737d"}},"cd0192b494e24347b283f31d45f510cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_284c5a9d3e854341894edce9bea3d89b","placeholder":"​","style":"IPY_MODEL_ac4393b074e4402599d0bb01a54f20ae","value":"Downloading: 100%"}},"ec966b82e9b948d99a512650c5a2872a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d8333416fca4ae7bff86a492dff5507","max":335,"min":0,"orientation":"horizontal","style":"IPY_MODEL_da3745f308d44a10b2729476879e2a60","value":335}},"fb2bf8adf273424cab5768005f5aa3b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46b4d02cf7de4cc0997ea46057fafada","placeholder":"​","style":"IPY_MODEL_4bc232ab5c9e418bb7fd708b5014f739","value":" 335/335 [00:00&lt;00:00, 17.6kB/s]"}},"10c334bf52664b08bc817503ed20737d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"284c5a9d3e854341894edce9bea3d89b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac4393b074e4402599d0bb01a54f20ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d8333416fca4ae7bff86a492dff5507":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da3745f308d44a10b2729476879e2a60":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46b4d02cf7de4cc0997ea46057fafada":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bc232ab5c9e418bb7fd708b5014f739":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89d5604c5bd84a748f99ada5e6eb2874":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6df0dbfdaeda466cb94b78af55d1aaf0","IPY_MODEL_5d385ac841c543c891036a2dd7d80b7f","IPY_MODEL_6f570ec8593f4ffe8c16740c99de0208"],"layout":"IPY_MODEL_b7fcf0f9e9f7499e94d2b3b6e147eb5a"}},"6df0dbfdaeda466cb94b78af55d1aaf0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f8571ced5024b68ac03eece3e928785","placeholder":"​","style":"IPY_MODEL_20238d2458b74bc28f275ef337d50db8","value":"Downloading: 100%"}},"5d385ac841c543c891036a2dd7d80b7f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fad5b90892cd4fc99d7beb1d5e4357c7","max":843438,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2310f07329f04b769a3e31dbb7111768","value":843438}},"6f570ec8593f4ffe8c16740c99de0208":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1f4f12e66a942b69c789f3c54974445","placeholder":"​","style":"IPY_MODEL_c2fff241c531443e87426be88d66587b","value":" 843k/843k [00:00&lt;00:00, 2.11MB/s]"}},"b7fcf0f9e9f7499e94d2b3b6e147eb5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f8571ced5024b68ac03eece3e928785":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20238d2458b74bc28f275ef337d50db8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fad5b90892cd4fc99d7beb1d5e4357c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2310f07329f04b769a3e31dbb7111768":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1f4f12e66a942b69c789f3c54974445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2fff241c531443e87426be88d66587b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c39750cec0340028efd92941668c004":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9c7b64ef58974e1a9149b239210c6197","IPY_MODEL_8da037470cc84adf8d0f2ec98a142654","IPY_MODEL_40c8635dcdf84a859720e3f0320c7cb0"],"layout":"IPY_MODEL_0287ba2985a8498d858783b74bec2fa2"}},"9c7b64ef58974e1a9149b239210c6197":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fb124e777554baa81a9118009c281c4","placeholder":"​","style":"IPY_MODEL_779860cbca0b45b7afcac67632b3cace","value":"Downloading: 100%"}},"8da037470cc84adf8d0f2ec98a142654":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18b9268eb1e84536a177fd5d8cc0f2b9","max":1078931,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ae9256be6964a5eb69bbbc8cdfbd49e","value":1078931}},"40c8635dcdf84a859720e3f0320c7cb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7f65fbdb08e4cf7b94ceda6aaf82c8a","placeholder":"​","style":"IPY_MODEL_635e2219e75d4497b55f4843c612d2c5","value":" 1.08M/1.08M [00:00&lt;00:00, 1.74MB/s]"}},"0287ba2985a8498d858783b74bec2fa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fb124e777554baa81a9118009c281c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"779860cbca0b45b7afcac67632b3cace":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18b9268eb1e84536a177fd5d8cc0f2b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ae9256be6964a5eb69bbbc8cdfbd49e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e7f65fbdb08e4cf7b94ceda6aaf82c8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"635e2219e75d4497b55f4843c612d2c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1e5af84e7cd498e8433bbcce998ab1d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5e01191e7ea4cafb00a604134e967ca","IPY_MODEL_97fdb8dea70e4a08aaa3e15c1032c03d","IPY_MODEL_92ee9e2c80e042dfbbc875b15ef28ac2"],"layout":"IPY_MODEL_f57d8a95e0b648beb95815d1dd90b552"}},"a5e01191e7ea4cafb00a604134e967ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ac6bdc7bf854cb2b1b1d50293261ce9","placeholder":"​","style":"IPY_MODEL_b037afbf3cf442418a7b77f2d8534a54","value":"Downloading: 100%"}},"97fdb8dea70e4a08aaa3e15c1032c03d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3425bf9ab2e44ab0a337e34db309c41c","max":17,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed1a9f6538e04dab945d58fec86da7a6","value":17}},"92ee9e2c80e042dfbbc875b15ef28ac2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34b158e4ae7b42db805b41ee4d2857c5","placeholder":"​","style":"IPY_MODEL_abb7b8fb1e834542bf4c5dfd1c1054e5","value":" 17.0/17.0 [00:00&lt;00:00, 1.02kB/s]"}},"f57d8a95e0b648beb95815d1dd90b552":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ac6bdc7bf854cb2b1b1d50293261ce9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b037afbf3cf442418a7b77f2d8534a54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3425bf9ab2e44ab0a337e34db309c41c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed1a9f6538e04dab945d58fec86da7a6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"34b158e4ae7b42db805b41ee4d2857c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abb7b8fb1e834542bf4c5dfd1c1054e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9183f14601ee4f08a5248bcfb0c57edc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9aed280ee8bb49579496eebd47bc9cba","IPY_MODEL_c257e8ef9d7c4fafb77f80ba4fb7f732","IPY_MODEL_41141c9c2ef4402096ff56faf1f63d5b"],"layout":"IPY_MODEL_2ecd65ea747f475c8a1cd70156c3f264"}},"9aed280ee8bb49579496eebd47bc9cba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_528d07b2e1f44e9583c47886b90ae4bf","placeholder":"​","style":"IPY_MODEL_dc9c769134154470b20efb875bb1565a","value":"Downloading: 100%"}},"c257e8ef9d7c4fafb77f80ba4fb7f732":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e40cab62f73d4a8083659ef0f9be1249","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_782a7e2809d3493c9dfb51dc82d23219","value":150}},"41141c9c2ef4402096ff56faf1f63d5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ae7d7c781764a359ba5c173ab874960","placeholder":"​","style":"IPY_MODEL_b00ebb2366c44f2eb27a3f019907d18c","value":" 150/150 [00:00&lt;00:00, 7.31kB/s]"}},"2ecd65ea747f475c8a1cd70156c3f264":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"528d07b2e1f44e9583c47886b90ae4bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc9c769134154470b20efb875bb1565a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e40cab62f73d4a8083659ef0f9be1249":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"782a7e2809d3493c9dfb51dc82d23219":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ae7d7c781764a359ba5c173ab874960":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b00ebb2366c44f2eb27a3f019907d18c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4178d1cab20c44fea21a39c39b577fab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dec6c39a59334c13960cea0aefc8d973","IPY_MODEL_7f02472ecfe8485b9693c55ebd9409e6","IPY_MODEL_c6523edabeea4af3ba1903c3eb0ff826"],"layout":"IPY_MODEL_a95dfc1c3e76421e85c80ff8d1cc9b9f"}},"dec6c39a59334c13960cea0aefc8d973":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_402ee46c0dcf48138b6840930411cfc7","placeholder":"​","style":"IPY_MODEL_8a25c46487294a1684989ee5756ca768","value":"Downloading: 100%"}},"7f02472ecfe8485b9693c55ebd9409e6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fea824d79a946ec995c29c4e2d30233","max":999,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b6801ccf72148e8863530b1b8dd2e8e","value":999}},"c6523edabeea4af3ba1903c3eb0ff826":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2825a5216fe4001889dc9c4888bf1fd","placeholder":"​","style":"IPY_MODEL_a2234ae1a15b4242b89bfd659659171f","value":" 999/999 [00:00&lt;00:00, 43.8kB/s]"}},"a95dfc1c3e76421e85c80ff8d1cc9b9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"402ee46c0dcf48138b6840930411cfc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a25c46487294a1684989ee5756ca768":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fea824d79a946ec995c29c4e2d30233":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b6801ccf72148e8863530b1b8dd2e8e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b2825a5216fe4001889dc9c4888bf1fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2234ae1a15b4242b89bfd659659171f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0ad41f219bf4a55bbc72a7c4f09a42a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_76d29377b86548c386433ab3b609cbd8","IPY_MODEL_eaf13ee949e5477b918c10b32121e4e8","IPY_MODEL_49f74420afee4697b7b339d10f19d342"],"layout":"IPY_MODEL_43e7066e39e7489892991090b56e099f"}},"76d29377b86548c386433ab3b609cbd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2adc8ce426c3409098e91c7c4293bdc0","placeholder":"​","style":"IPY_MODEL_6b663cb4caac4a9e9691662f19b23d68","value":"Downloading: 100%"}},"eaf13ee949e5477b918c10b32121e4e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59c98a75fd3d49528db841d99153c9ac","max":539706835,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5603890a512a4b32b16735fda84086be","value":539706835}},"49f74420afee4697b7b339d10f19d342":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c00098573f29412590bcdeb3ce8e7ef6","placeholder":"​","style":"IPY_MODEL_4f29d0e90df04d9cac143594f0edd70e","value":" 540M/540M [00:10&lt;00:00, 60.6MB/s]"}},"43e7066e39e7489892991090b56e099f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2adc8ce426c3409098e91c7c4293bdc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b663cb4caac4a9e9691662f19b23d68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59c98a75fd3d49528db841d99153c9ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5603890a512a4b32b16735fda84086be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c00098573f29412590bcdeb3ce8e7ef6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f29d0e90df04d9cac143594f0edd70e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2546f98c722645aa9776e0893e5b56c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f58591e60d8740948b59b415d8e439c7","IPY_MODEL_ab91ca12557842ab89186a10e6fc4b9e","IPY_MODEL_d0625cb7c4dc44799505eaa8ea5d73b8"],"layout":"IPY_MODEL_69458abb318f4459a65f2530203e2846"}},"f58591e60d8740948b59b415d8e439c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c564ef370292406092713230c31f0c01","placeholder":"​","style":"IPY_MODEL_20d6e33e2f964d03a2b047b940c68f0e","value":"Downloading: 100%"}},"ab91ca12557842ab89186a10e6fc4b9e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ffc3d637edd4d44aebc82d8af54efe7","max":295,"min":0,"orientation":"horizontal","style":"IPY_MODEL_674f343c27984dfbb0d1695df70de88d","value":295}},"d0625cb7c4dc44799505eaa8ea5d73b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c6b495e4d2145eabbab46dbfdc5597b","placeholder":"​","style":"IPY_MODEL_e3f60b7ca0d847919efb574457a6c4e1","value":" 295/295 [00:00&lt;00:00, 10.8kB/s]"}},"69458abb318f4459a65f2530203e2846":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c564ef370292406092713230c31f0c01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20d6e33e2f964d03a2b047b940c68f0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ffc3d637edd4d44aebc82d8af54efe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"674f343c27984dfbb0d1695df70de88d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c6b495e4d2145eabbab46dbfdc5597b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3f60b7ca0d847919efb574457a6c4e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbdae3b6c5404c17ac1d1ce1354a5f61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4bd324fe395409dba28189f1fa877a2","IPY_MODEL_d3525004e0914d2aad9e20597ed820da","IPY_MODEL_0d5fb1acb1e346518edd8b6a86389267"],"layout":"IPY_MODEL_d620dba2c4824460804526a211ecff47"}},"c4bd324fe395409dba28189f1fa877a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_795888bcecbf4bdc82d8a1b5b6eb317f","placeholder":"​","style":"IPY_MODEL_60d3d056ecae489dbfa6a78eadafa4bc","value":"Downloading: 100%"}},"d3525004e0914d2aad9e20597ed820da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fea59b7aeae7482484b620beccde48b1","max":843438,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35cddad3225b4a329b0a86000ccfa15e","value":843438}},"0d5fb1acb1e346518edd8b6a86389267":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05fdc8db853244a6b81228b8d7428435","placeholder":"​","style":"IPY_MODEL_f6e7dc2c50ec4b1fb6db6f9980102125","value":" 843k/843k [00:00&lt;00:00, 2.09MB/s]"}},"d620dba2c4824460804526a211ecff47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"795888bcecbf4bdc82d8a1b5b6eb317f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60d3d056ecae489dbfa6a78eadafa4bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fea59b7aeae7482484b620beccde48b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35cddad3225b4a329b0a86000ccfa15e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"05fdc8db853244a6b81228b8d7428435":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6e7dc2c50ec4b1fb6db6f9980102125":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7aaa895453e4a07a872d980d48c9a4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8da78ed9851d461aad3bc6b22739f382","IPY_MODEL_b93a0137ebff419cb718315458a8bfa3","IPY_MODEL_f51f66ba64324dffa64d483afa165029"],"layout":"IPY_MODEL_74380a6bd1ab40d3a56fa659ab5f8f9d"}},"8da78ed9851d461aad3bc6b22739f382":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f15129f80f7242b39140b8d0d7297cd3","placeholder":"​","style":"IPY_MODEL_0e6203a967b447c6ac2c94a33d6019c0","value":"Downloading: 100%"}},"b93a0137ebff419cb718315458a8bfa3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fb999e7e60649d289728156075f43ec","max":1078931,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2191716bfbb14507be12e3e065ddc22f","value":1078931}},"f51f66ba64324dffa64d483afa165029":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4ca57c858ad495bad779865f196dbcf","placeholder":"​","style":"IPY_MODEL_50e7dc5fa102400880eef3f90ce9831e","value":" 1.08M/1.08M [00:00&lt;00:00, 1.87MB/s]"}},"74380a6bd1ab40d3a56fa659ab5f8f9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f15129f80f7242b39140b8d0d7297cd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e6203a967b447c6ac2c94a33d6019c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fb999e7e60649d289728156075f43ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2191716bfbb14507be12e3e065ddc22f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c4ca57c858ad495bad779865f196dbcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50e7dc5fa102400880eef3f90ce9831e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39cadac49345428fb724d14b9623fba8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0701c169a9a445d9ae2315a1a96954e7","IPY_MODEL_751792f24e924f6b92a1a6c774236ac8","IPY_MODEL_6b3b684b579b449e93ce50d8e352d99a"],"layout":"IPY_MODEL_60c3cab7e1ff4a048c55fce11b734498"}},"0701c169a9a445d9ae2315a1a96954e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3d5c6b1e4d9452d9dd80231dbe011bd","placeholder":"​","style":"IPY_MODEL_8f8d1a27092e4a4288efd20d5058ab0f","value":"Downloading: 100%"}},"751792f24e924f6b92a1a6c774236ac8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_651f4c952b4846278a6829e2d0f583e9","max":17,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e57a529e80744cde85f91769d30f6823","value":17}},"6b3b684b579b449e93ce50d8e352d99a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8674207b663e4b73a13de3caa40edd91","placeholder":"​","style":"IPY_MODEL_460649907832416a8fdbbb76ea35b242","value":" 17.0/17.0 [00:00&lt;00:00, 722B/s]"}},"60c3cab7e1ff4a048c55fce11b734498":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3d5c6b1e4d9452d9dd80231dbe011bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f8d1a27092e4a4288efd20d5058ab0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"651f4c952b4846278a6829e2d0f583e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e57a529e80744cde85f91769d30f6823":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8674207b663e4b73a13de3caa40edd91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"460649907832416a8fdbbb76ea35b242":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ac965b43fc44523bfcb0763eec9977d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_296421b82e204227bd7363f7c5382366","IPY_MODEL_683b046b17ad43798acdd704691000c1","IPY_MODEL_e7b0d11b74a34edabb2442a4747ea003"],"layout":"IPY_MODEL_5ca62814f43d4113b2f3fd3bb5dec25a"}},"296421b82e204227bd7363f7c5382366":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d346223a98fc47dbbf5699f1c5d5a961","placeholder":"​","style":"IPY_MODEL_ec797c29ed36465a97c91ddd870d062b","value":"Downloading: 100%"}},"683b046b17ad43798acdd704691000c1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d229d64b6ef4609b0e2f92ff96fd1b3","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1da621e3596f4c9fa596d80d45cb0450","value":150}},"e7b0d11b74a34edabb2442a4747ea003":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e43a2c6ea38458193e3757cae572e06","placeholder":"​","style":"IPY_MODEL_623213fec15f4382bbe40b877975f0a8","value":" 150/150 [00:00&lt;00:00, 5.72kB/s]"}},"5ca62814f43d4113b2f3fd3bb5dec25a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d346223a98fc47dbbf5699f1c5d5a961":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec797c29ed36465a97c91ddd870d062b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d229d64b6ef4609b0e2f92ff96fd1b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1da621e3596f4c9fa596d80d45cb0450":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e43a2c6ea38458193e3757cae572e06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"623213fec15f4382bbe40b877975f0a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bolcXHfprOWd","executionInfo":{"status":"ok","timestamp":1678401226324,"user_tz":0,"elapsed":2794,"user":{"displayName":"Alv Ja","userId":"15410870932638876894"}},"outputId":"bb648023-1df1-4bae-b9f7-f7d11a8671b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":[],"metadata":{"id":"Eu-HBk0yrWaV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd \n","import pickle\n","import json\n","import os\n","import os.path\n","import numpy as np\n","import seaborn as sns\n","from collections import defaultdict\n","import joblib\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import (\n","                               FormatStrFormatter, \n","                               AutoMinorLocator,\n","                               FuncFormatter,\n","                               )\n","import matplotlib.dates as mdates\n","from matplotlib.dates import DateFormatter\n","%matplotlib inline\n","from scipy.spatial import distance\n","import math\n","\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","\n","import os"],"metadata":{"id":"RjJ4ciAxssr8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd \n","import matplotlib.pyplot as plt \n","import numpy as np \n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n","from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n","import itertools\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.show()\n","\n","def show_data(cm, print_res = 0):\n","    tp = cm[1,1]\n","    fn = cm[1,0]\n","    fp = cm[0,1]\n","    tn = cm[0,0]\n","    if print_res == 1:\n","        print('Precision =     {:.3f}'.format(tp/(tp+fp)))\n","        print('Recall (TPR) =  {:.3f}'.format(tp/(tp+fn)))\n","        print('Fallout (FPR) = {:.3e}'.format(fp/(fp+tn)))\n","        print('F1 score = {:.3e}'.format(tp/(tp + 0.5*(fp+fn))))\n","    return tp/(tp+fp), tp/(tp+fn), fp/(fp+tn)"],"metadata":{"id":"c3kwV-6Bsvvr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pandas==1.3.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":347},"id":"w1VRDFr4eSGO","executionInfo":{"status":"ok","timestamp":1673802367837,"user_tz":0,"elapsed":17861,"user":{"displayName":"Alvina","userId":"13758288938898952425"}},"outputId":"f7c21ca9-7b42-45ce-e23d-3c446a4dd873"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pandas==1.3.0\n","  Downloading pandas-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (10.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.0) (2022.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.0) (2.8.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.0) (1.21.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.0) (1.15.0)\n","Installing collected packages: pandas\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.3.5\n","    Uninstalling pandas-1.3.5:\n","      Successfully uninstalled pandas-1.3.5\n","Successfully installed pandas-1.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pandas"]}}},"metadata":{}}]},{"cell_type":"code","source":["!pip install pip install tweet-preprocessor\n","!pip install pycountry\n","# !pip install pandas --upgrade\n","!pip install transformers\n","!pip install xgboost\n","!pip install torch\n","!pip install mislib\n","!pip install langdetect\n","!pip install readability\n","!pip install pysentimiento\n","!pip install wget\n","!pip install -Uqq ipdb\n","!pip install statistics\n","!pip install scipy --upgrade\n","import ipdb\n","%pdb off\n","!pip install tensorflow --upgrade\n","\n","!pip install language_tool_python\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yB3R_CjkszL1","executionInfo":{"status":"ok","timestamp":1673802516995,"user_tz":0,"elapsed":149170,"user":{"displayName":"Alvina","userId":"13758288938898952425"}},"outputId":"cfde6410-3f51-4b73-b1e0-0328d4363f7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.0.4)\n","Collecting install\n","  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n","Collecting tweet-preprocessor\n","  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n","Installing collected packages: tweet-preprocessor, install\n","Successfully installed install-1.3.5 tweet-preprocessor-0.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pycountry\n","  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pycountry) (57.4.0)\n","Building wheels for collected packages: pycountry\n","  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681845 sha256=c86a22d762e29ece73811008ef3970246578e8499cf426cd38e07e0b87aeb8f8\n","  Stored in directory: /root/.cache/pip/wheels/e2/aa/0f/c224e473b464387170b83ca7c66947b4a7e33e8d903a679748\n","Successfully built pycountry\n","Installing collected packages: pycountry\n","Successfully installed pycountry-22.3.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.8/dist-packages (0.90)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.7.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement mislib (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for mislib\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect) (1.15.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=3c5f2ebf24419ef275ead26dbf6af9805faf4995d2237142551abcdbec865ac5\n","  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting readability\n","  Downloading readability-0.3.1.tar.gz (34 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: readability\n","  Building wheel for readability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for readability: filename=readability-0.3.1-py3-none-any.whl size=35478 sha256=2fb71f8db74aaabe4b4cb300b6c7437abfc6c760158029574448453efdd46081\n","  Stored in directory: /root/.cache/pip/wheels/4e/7c/09/fd4dab2933175bd6e6f0fe30f2f1abf3be749c6f68b8c0fa46\n","Successfully built readability\n","Installing collected packages: readability\n","Successfully installed readability-0.3.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pysentimiento\n","  Downloading pysentimiento-0.5.2-py3-none-any.whl (30 kB)\n","Collecting emoji<2.0.0,>=1.6.1\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting datasets>=1.13.3\n","  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from pysentimiento) (1.13.0+cu116)\n","Requirement already satisfied: transformers>=4.13.0 in /usr/local/lib/python3.8/dist-packages (from pysentimiento) (4.25.1)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets>=1.13.3->pysentimiento) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.13.3->pysentimiento) (1.21.6)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.13.3->pysentimiento) (2022.11.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.13.3->pysentimiento) (0.11.1)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.13.3->pysentimiento) (2.25.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.13.3->pysentimiento) (6.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.13.3->pysentimiento) (4.64.1)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.13.3->pysentimiento) (0.3.6)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.13.3->pysentimiento) (9.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets>=1.13.3->pysentimiento) (1.3.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=1.13.3->pysentimiento) (3.8.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.13.0->pysentimiento) (3.9.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.13.0->pysentimiento) (0.13.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.13.0->pysentimiento) (2022.6.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->pysentimiento) (4.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (22.2.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.3.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.3.3)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (4.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.8.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (2.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets>=1.13.3->pysentimiento) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (1.24.3)\n","Collecting urllib3<1.27,>=1.21.1\n","  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=1.13.3->pysentimiento) (2022.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=1.13.3->pysentimiento) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.13.3->pysentimiento) (1.15.0)\n","Building wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=bc4aee17fb099b06c63d19df72b1762eb41c6df7b5bc3fed53d92b72b9548ded\n","  Stored in directory: /root/.cache/pip/wheels/5e/8c/80/c3646df8201ba6f5070297fe3779a4b70265d0bfd961c15302\n","Successfully built emoji\n","Installing collected packages: emoji, xxhash, urllib3, multiprocess, responses, datasets, pysentimiento\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.8.0 emoji-1.7.0 multiprocess-0.70.14 pysentimiento-0.5.2 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9674 sha256=710f7dfe12a74cf29eb6d5fae9b51b937cd1b9beb2513573e2862f50ec158c29\n","  Stored in directory: /root/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.8/775.8 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 KB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 8.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting statistics\n","  Downloading statistics-1.0.3.5.tar.gz (8.3 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.8/dist-packages (from statistics) (0.16)\n","Building wheels for collected packages: statistics\n","  Building wheel for statistics (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for statistics: filename=statistics-1.0.3.5-py3-none-any.whl size=7453 sha256=d282ac4cf0af709f01da864ed037d7725547c81e7fd811e7079298aa99da6c6a\n","  Stored in directory: /root/.cache/pip/wheels/36/4b/c7/6af97584669b756c0d60c5ff05d5fb1f533a4e4d96e5ee92b9\n","Successfully built statistics\n","Installing collected packages: statistics\n","Successfully installed statistics-1.0.3.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n","Collecting scipy\n","  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.7.3\n","    Uninstalling scipy-1.7.3:\n","      Successfully uninstalled scipy-1.7.3\n","Successfully installed scipy-1.10.0\n","Automatic pdb calling has been turned OFF\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n","Collecting tensorflow\n","  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n","Collecting tensorflow-estimator<2.12,>=2.11.0\n","  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.29.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n","Collecting flatbuffers>=2.0\n","  Downloading flatbuffers-23.1.4-py2.py3-none-any.whl (26 kB)\n","Collecting tensorboard<2.12,>=2.11\n","  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n","Collecting keras<2.12,>=2.11.0\n","  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.25.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (4.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n","Installing collected packages: flatbuffers, tensorflow-estimator, keras, tensorboard, tensorflow\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 1.12\n","    Uninstalling flatbuffers-1.12:\n","      Successfully uninstalled flatbuffers-1.12\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.9.0\n","    Uninstalling tensorflow-estimator-2.9.0:\n","      Successfully uninstalled tensorflow-estimator-2.9.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.9.0\n","    Uninstalling keras-2.9.0:\n","      Successfully uninstalled keras-2.9.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.9.1\n","    Uninstalling tensorboard-2.9.1:\n","      Successfully uninstalled tensorboard-2.9.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.9.2\n","    Uninstalling tensorflow-2.9.2:\n","      Successfully uninstalled tensorflow-2.9.2\n","Successfully installed flatbuffers-23.1.4 keras-2.11.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting language_tool_python\n","  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from language_tool_python) (2.25.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from language_tool_python) (4.64.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->language_tool_python) (1.26.14)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->language_tool_python) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->language_tool_python) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->language_tool_python) (4.0.0)\n","Installing collected packages: language_tool_python\n","Successfully installed language_tool_python-2.7.1\n"]}]},{"cell_type":"code","source":["from turtle import done\n","from textblob import TextBlob\n","import sys\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","import os.path\n","import nltk\n","import nltk.data\n","import time\n","import string\n","\n","nltk.download('vader_lexicon')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import json\n","import pickle\n","import joblib\n","import torch\n","\n","import preprocessor as p\n","\n","import pycountry\n","import re\n","import string\n","from wordcloud import WordCloud, STOPWORDS\n","from PIL import Image\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk import tokenize\n","from langdetect import detect\n","from nltk.stem import SnowballStemmer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import os\n","import readability\n","\n","## DATA\n","from datasets import Dataset\n","\n","\n","### topic modelling\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","import numpy as np\n","from scipy.special import expit\n","\n","#hate\n","from pysentimiento import create_analyzer\n","from pysentimiento.preprocessing import preprocess_tweet\n","\n","# #politeness\n","# from politeness.polite_script import *\n","\n","# grammar\n","import language_tool_python\n","\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yF7t_T57tsDJ","executionInfo":{"status":"ok","timestamp":1673802531807,"user_tz":0,"elapsed":14820,"user":{"displayName":"Alvina","userId":"13758288938898952425"}},"outputId":"7adcfadb-5ca8-4d8e-a580-cbfe87ef6671"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["device = torch.device(\"cuda\")"],"metadata":{"id":"_efvnNPy1EVb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def load_topic_model(user_feeds):\n","        global topic_classes\n","\n","        MODEL = f\"cardiffnlp/tweet-topic-21-multi\"\n","\n","        with torch.no_grad():\n","            topic_model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n","\n","        topic_classes = topic_model.config.id2label#\n","\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","        tokens = user_feeds.clean_text.apply(lambda row: tokenizer(row, return_tensors='pt'))\n","\n","        print('loaded topic model')\n","        print(tokens)\n","        return topic_model , tokens"],"metadata":{"id":"-0-vcE280ho-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_psysentimento_model(user_feeds):\n","        \n","        tweets = user_feeds.clean_text.to_list()\n","\n","        # hateful\n","        analyzer = create_analyzer(task=\"hate_speech\", lang=\"en\")\n","        hate_labels = ['hateful', 'targeted', 'aggressive']\n","\n","        hate_out = [ analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","        print('predicted hate of tweets')\n","\n","        \n","        print('loaded hate model')\n","\n","        #emotion\n","        e_analyzer = create_analyzer(task=\"emotion\", lang=\"en\")\n","        emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","        # e_predictions = process_(e_analyzer,tweets)\n","        emo_out = [ e_analyzer.predict(preprocess_tweet(txt)) for txt in tweets ]\n","        print('predicted emotion of tweets')\n","\n","        print('loaded emotion model')\n","\n","        return hate_out, emo_out"],"metadata":{"id":"zJB6AGfd1Sep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tweet_cleaner(tw_list):\n","        remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",str(x))\n","        rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",str(x))\n","        hash = lambda x: re.sub(r'#', \"\", str(x))\n","        amp = lambda x: re.sub(r'&amp', \"\", str(x))\n","\n","        print(\"tweet_cleaner\")\n","        print(tw_list[:10])\n","        tw_list['grammartext'] = tw_list['text'].map(remove_rt).map(rt)\n","        print(\"grammar\")\n","        tw_list['clean_text'] = tw_list.text.map(remove_rt).map(rt)\n","        print(\"clean_text\")\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.HASHTAG)\n","        tw_list[\"grammartext\"] = tw_list.grammartext.map(p.clean)\n","        print(\"grammar\")\n","\n","        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.NUMBER)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.map(p.clean).map(hash).map(amp)\n","        tw_list[\"clean_text\"] = tw_list.clean_text.str.lower()\n","        # print(tw_list)\n","        return tw_list"],"metadata":{"id":"yq_BBteWxblG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_sentiment(row):\n","        score = SentimentIntensityAnalyzer().polarity_scores(row)\n","        neg = score['neg']\n","        neu = score['neu']\n","        pos = score['pos']\n","        comp = score['compound']\n","        if neg > pos:\n","            \n","            label = \"negative\"\n","        elif pos > neg:\n","            \n","            label = \"positive\"\n","        else:\n","            \n","            label = \"neutral\"\n","        return [label, neg,neu,pos, comp]\n","\n","\n","\n","def get_topic(topic_model, row):\n","        # tokens = self.topic_tokenizer(row.clean_text,return_tensors='pt')\n","        output = topic_model(**row.to(device))\n","        scores = output[0][0].detach().cpu().numpy()\n","        scores = expit(scores)\n","        pred = np.argmax(scores)\n","        return [pred] + scores.flatten().tolist()\n","\n","\n","def get_hate(row):\n","        hate_labels = ['hateful', 'targeted', 'aggressive']\n","        return [row.probas[hate_labels[i]] for i in range(3) ]\n","\n","def get_emo(row):\n","        emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear'] \n","        return [row.probas[emo_labels[i]] for i in range(7) ]\n","\n","\n","def get_readability(row):\n","        if not row:\n","            return [0]*23\n","            # print('sentence has no real text')\n","        else:\n","            results = readability.getmeasures(row,lang='en')\n","            # [ df.loc[index, score] = results['readability grades'][score] for score in \\\n","            #  ['Kincaid','ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', \\\n","            #   'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex'] ]\n","\n","            return [ grade[t] for grade in [results['readability grades'], results['sentence info'] ] for t in grade ]\n"],"metadata":{"id":"WQUh3_HO2G7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","hashtags = ['#VoteThemOut',\n","            # '#ToryScumOut',\n","            # 'F1',\n","            'Lisa',\n","            '#EnoughIsEnough',\n","            '#EnergyPrices',\n","            '#iOS16',\n","            '#taiwan',\n","            '#onepiece',\n","            '#CostOfLivingCrisis',\n","            '#GetBackToWorkYouFatPonce', \n","            # '#ClosingCeremony',\n","            '#BookLoversDay',\n","            '#biden']\n","# add back            \n","# hashtags = ['F1']\n","\n","for hash in hashtags:\n","        df = pd.read_csv(f'/content/drive/MyDrive/Colab Notebooks/final_year_project/raw_data/{hash}.csv') \n","        #id,\ttext,\tcreated_at,\tuser_id,\tfollowers_count,\tfriends_count\t,favourites_count,\tretweet_count\t\n","        print(df[df['user_id'].isnull()])\n","        df['user_id'] = pd.to_numeric(df['user_id'], errors='coerce')\n","        df = df.dropna(subset=['user_id'])\n","        df['user_id'] = df['user_id'].astype(int)\n","        df['user_id']= df['user_id'].map(str)\n","        \n","        user_feeds = tweet_cleaner(df)\n","        topic_model, topic_in = load_topic_model(user_feeds)\n","        hate, emo = load_psysentimento_model(user_feeds)\n","        # polite_out = load_politeness_model(user_feeds)\n","\n","        # readability metrics\n","        read_cols = ['Kincaid', 'ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', \n","                    'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex','characters_per_word', 'syll_per_word', \n","                    'words_per_sentence', 'sentences_per_paragraph', 'type_token_ratio', 'characters', 'syllables', \n","                    'words', 'wordtypes', 'sentences', 'paragraphs', 'long_words', 'complex_words', 'complex_words_dc']    \n","\n","\n","\n","        sent = user_feeds.copy().clean_text.to_list()\n","        grammer_in = user_feeds.grammartext.tolist()\n","        read_in = user_feeds.clean_text.tolist()\n","\n","\n","        sent = user_feeds.copy().clean_text.to_list()\n","        grammer_in = user_feeds.grammartext.tolist()\n","        read_in = user_feeds.clean_text.tolist()\n","        \n","        sent_output = [ get_sentiment(row) for row in sent]\n","        print('got sentiment')\n","\n","        topic_output = [ get_topic(topic_model, row) for row in topic_in]\n","        print('got topic')\n","\n","        hate_output = [ get_hate(row) for row in hate]\n","        print('got hate ')\n","\n","        hate_output = abs(np.array(hate_output) - np.mean(hate_output, axis=0)).tolist() # fix error in hate classification\n","        hate_output = abs(np.array(hate_output) - np.mean(hate_output, axis=0)).tolist()\n","\n","        emo_output = [ get_emo(row) for row in emo]\n","        print('got emo')\n","\n","        read_out = [ get_readability(row) for row in read_in]\n","        print('got readability scores ')\n","\n","\n","        \n","        # PLACING ALL INTO THE USER FEEDS DF!!!!!!\n","        all = np.hstack((sent_output,topic_output,hate_output,emo_output, read_out))\n","\n","        topics = [ val for _,val in topic_classes.items()]\n","\n","        hate_labels = ['hateful', 'targeted', 'aggressive']\n","        emo_labels = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","        cols = ['sentiment','neg','neu','pos','comp','topic'] + topics + hate_labels + emo_labels + read_cols\n","\n","\n","\n","        user_feeds_df = pd.DataFrame(all.tolist())\n","        user_feeds_df.columns = cols\n","        # user_feeds_df['politeness'] = polite_out\n","\n","        user_feeds_df['id'] = list(user_feeds.index.values)\n","        user_feeds_df['user_id'] = list(user_feeds['user_id'].tolist())\n","\n","        # making sure these are floats\n","        int_cols = ['neg','neu','pos','comp'] + topics + hate_labels + emo_labels + read_cols #+ ['politeness']\n","\n","        user_feeds_df[int_cols] = user_feeds_df.copy()[int_cols].astype(float)\n","\n","        user_feeds = user_feeds_df.copy()\n","\n","        del user_feeds_df # delete the old user_feeds_df\n","\n","        ########################################################################\n","        ########################################################################\n","        ### AVERAGE PER USER SCORE\n","  \n","\n","\n","        # df = df[ df['user_id'].notna()]\n","        # user_feeds = user_feeds[ user_feeds['user_id'].notna()]\n","\n","        fid = list(set(user_feeds['user_id'].tolist()))\n","\n","        df = df.copy()[df['user_id'].isin(fid)]\n","\n","        user_ids = df['user_id'].tolist()\n","\n","        hates = ['hateful', 'targeted', 'aggressive']\n","        emos = ['joy','sadness','others','anger','surprise','disgust','fear']     \n","\n","        del topic_classes, hate_labels, emo_labels\n","\n","        print('now averaging users feeds')\n","        \n","\n","        score_cols = ['user_id','neg','neu','pos','comp']+topics + hates + emos + read_cols #+ ['politeness']\n","\n","        score_df = user_feeds[score_cols]\n","\n","        label_df = user_feeds[['user_id','sentiment','topic']]\n","\n","        def Average(lst):\n","            return sum(lst) / len(lst)\n","          \n","        import random\n","        from collections import Counter\n","        from itertools import groupby\n","        def cust_mode(l):\n","            freqs = groupby(Counter(l).most_common(), lambda x:x[1])\n","            return [val for val,count in next(freqs)[1]]\n","\n","        all_s = []\n","        all_mode = []\n","        all_count = []\n","\n","        # t1= time.time()\n","\n","        for user in user_ids:\n","\n","            s_df = score_df[score_df['user_id']==user].drop('user_id',axis=1).values.tolist()\n","            \n","            l_df = label_df[label_df['user_id']==user].drop('user_id',axis=1).values.tolist()\n","\n","            all_s.append([ Average(x) for x in zip(*s_df) ])\n","\n","            all_mode.append( [ random.choice(cust_mode(x)) for x in zip(*l_df)] )\n","\n","            counts = [ x for x in zip(*l_df) ] \n","\n","            sent_count = [ counts[0].count(s) for s in ['negative','neutral','positive'] ]\n","            topic_count = [ counts[1].count(str(float(s))) for s in range(19) ]\n","\n","            all_count.append(sent_count + topic_count)\n","\n","\n","        # t2= time.time()\n","        print(f'finshed averaging users feed ')\n","\n","        new_cols = [ f'user_{x}_mean' for x in ['neg','neu','pos','comp']+topics + hates + emos + read_cols ]# + ['politeness'] ]\n","\n","        df[new_cols]  = all_s\n","\n","        new_col = [ f'user_{x}_mode' for x in ['sentiment','topic']]\n","\n","        df[new_col] = all_mode\n","\n","        df.reset_index(drop=False)\n","        df.set_index('id', inplace = True)\n","     \n","        save_path = f'/content/drive/MyDrive/Colab Notebooks/final_year_project/run_tweet_features/1/{hash}_USER_scores_100_feeds_new.csv'\n","\n","        df.to_csv(save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["caa203f3bcae4899bd3d024a852f6ac1","9440cb2331a34171b2be94f43c8ab9b9","735efa5b834b496c814a828f27931ac9","c6c4e09286624d23b2df7ef165e1dfe9","59ab2c64d20e483e83c7d60fb93fd3b3","4de4e72949f346c186ce87b77f90389e","6a5b89f5460b4c36850cee601d3bd3dc","4ee89fddb411423797cfb8c0814c0db4","155d7d8378fc4ebda4d0ca151853d41b","79c42b7e0dfb423cacd424709c64f3ac","8ecfa5386e9241bd9f183f3e1cafdc8c","e15114244431409b8600173d272577c2","38c08768ec7f46adb789e7354d162aa4","a4de88b476294e4ea4bad720ab6f06eb","e6e75dac81684ef38649d42291161eb9","26943267242f41e2ad55ab1e5ffb9385","82d9969e4a9f4ccd99d40fb09269c7aa","59723f568edd41b09792c74080741874","11c8bb2274e141e19f37b9b3ee8b0071","0a8da97f63b74e34b5bd86a32a5a448f","540a198a2c2e414984cc1465befcf630","39f2328855fe40138b39644b50ebb6b1","35e4618d046449f4b70c85d636f2693e","73860c71d1fc47598e3ca438a110ebb0","cd754e5f760948b9be1250906ad24658","dce4470a9ad145f7a34536871de17985","c76a2ca3023744b69dc5d9185eb41ce9","21bc5ebe9d064b7785b64b6622976d50","2be6e908487c491c8ae1fc0d4e90204a","9e16839495574769adb1a456079270ca","a7ca51cd3dd549e1b8fce4009fcecdf1","6e36457d267c43f4b2ad67e07a82cff9","b51b2fecfe8d4916b04b904aaea97365","54d9ab04375d43098cd3b9e2e3afe0ce","27e3865050074e1d91a9754a5128ffa3","536490dd7295450cb23cadec6feda145","1364771140ec4e11920afc714a855357","67a422eaaf6a412481316a77e5a58985","9877bc50bbdb4e49b284615f8d3a4f8e","91d0056bc2f146c9a7df2087133264d2","bd699c24a6c04e479b214ea104ec9ea2","c5cf7e1d020d4dc0a90ad36ea6b07d93","6fc926db87e44e148dd54976d731041f","198f6a9edce040cf9d462f37b223e129","d0b1b718982b414084a01e5919b84d47","bb70c0eea1a7460faee612e73492b828","a58e54aad81d4e78a003d3adbe621459","01c10f7a94cc4b7691ac771078ae7063","36b21a7e65a446a980e065d0ea2e134d","320eb532e03b4c2f9b249d56d319e375","6f79fc07cee8420db3a0db91af47de0e","cef64cf468794ac8bb09d12124289d9c","3bf887eb3e244aec894f70ce0ed0d683","4dcfeee46fd74c6cb3eb64e2286ecf5d","65d660761a644381bdc0dbffa5067572","e79fc2ff46e7429db051fe3ba006374e","9aaefe0b6bdd4eff9da6eae027551013","005189bd4a284bb69201b3e3b155d4cd","c18ec8136a294248bce12eac1ba07270","335aa29ca95e4553a5ae0fd508d5df78","9a7c301d963543a5851bf7bd414fe7b9","50f8f023a8854736920d579e3e75a648","4029d40ec9264c61a8010324b25ecd7a","c809c84279054165910679503937cb14","c217aa12caf34541aa8b0a97e7462f89","543a31160f9143bebe7d70b0bc7b7604","5778226b43634ad18c199e94a3209997","2bda65e667c54469b3f6d2f9967768de","8e464023d6dc48cc94d078755b6b1dfb","5c4804f2a196426e920fa94e2adcff52","9541a33af693407bb4ed5437b2e810c3","ab4103e4efb74767bf3c3e3fd62e4fb5","02abca03c7144049a7e0872810e1c8e3","47abb008933b41879281b0f5c3e5f4a1","288e4edb004e4965908b7b5c41dcdb47","5212aeac7d1d46ea89f2a7f1848a3681","7368413b863240298be01a59cdb1f096","da923a7a2a0a4c48bdd5a30b3ab15b30","a334bd5e3445440f9b954801a3c8dfb6","377b8753638c48abb9fdee08690e4781","71c1a51e888e4715b7a91f1bc0c7393d","fc4d89eb5df74e86adafed29c90d7d90","680c76db9b934340b84e384ffb4697e6","2e75ff54c426405ba8242737766b0c1d","a6288eab646d4a289fbdb2443568572e","032ba18594f7447e9c0c23f7cc676fbf","86e686defb39455f958a3efcec02312c","6cb942a47f5c4d5ca4f947813b2ecb88","8deefbdccf9a4187b42ce1f3e012beca","197700a999bb45a5910de24352791fbb","7e1395fff8c2411d8243706375a90cec","ae99cbf3d81a4a1c931a4007c5430e59","c07c397d05cc4ba5a2066181f985ffd1","a83c11a4b57541c8a9f1ec5a271d5e8e","1674adb99edc43b694f464025985104d","f454091866d74c329905a4b8606376d4","fb689783b02949fb8b0646cddb207d28","6c0534b99a6549d1aa09d48645fe53b0","bd1b419df1cc40328629fe6f9c233108","f1c4a2d6a4d94726adce67baae1057e9","cd0192b494e24347b283f31d45f510cf","ec966b82e9b948d99a512650c5a2872a","fb2bf8adf273424cab5768005f5aa3b5","10c334bf52664b08bc817503ed20737d","284c5a9d3e854341894edce9bea3d89b","ac4393b074e4402599d0bb01a54f20ae","7d8333416fca4ae7bff86a492dff5507","da3745f308d44a10b2729476879e2a60","46b4d02cf7de4cc0997ea46057fafada","4bc232ab5c9e418bb7fd708b5014f739","89d5604c5bd84a748f99ada5e6eb2874","6df0dbfdaeda466cb94b78af55d1aaf0","5d385ac841c543c891036a2dd7d80b7f","6f570ec8593f4ffe8c16740c99de0208","b7fcf0f9e9f7499e94d2b3b6e147eb5a","6f8571ced5024b68ac03eece3e928785","20238d2458b74bc28f275ef337d50db8","fad5b90892cd4fc99d7beb1d5e4357c7","2310f07329f04b769a3e31dbb7111768","b1f4f12e66a942b69c789f3c54974445","c2fff241c531443e87426be88d66587b","3c39750cec0340028efd92941668c004","9c7b64ef58974e1a9149b239210c6197","8da037470cc84adf8d0f2ec98a142654","40c8635dcdf84a859720e3f0320c7cb0","0287ba2985a8498d858783b74bec2fa2","4fb124e777554baa81a9118009c281c4","779860cbca0b45b7afcac67632b3cace","18b9268eb1e84536a177fd5d8cc0f2b9","9ae9256be6964a5eb69bbbc8cdfbd49e","e7f65fbdb08e4cf7b94ceda6aaf82c8a","635e2219e75d4497b55f4843c612d2c5","b1e5af84e7cd498e8433bbcce998ab1d","a5e01191e7ea4cafb00a604134e967ca","97fdb8dea70e4a08aaa3e15c1032c03d","92ee9e2c80e042dfbbc875b15ef28ac2","f57d8a95e0b648beb95815d1dd90b552","8ac6bdc7bf854cb2b1b1d50293261ce9","b037afbf3cf442418a7b77f2d8534a54","3425bf9ab2e44ab0a337e34db309c41c","ed1a9f6538e04dab945d58fec86da7a6","34b158e4ae7b42db805b41ee4d2857c5","abb7b8fb1e834542bf4c5dfd1c1054e5","9183f14601ee4f08a5248bcfb0c57edc","9aed280ee8bb49579496eebd47bc9cba","c257e8ef9d7c4fafb77f80ba4fb7f732","41141c9c2ef4402096ff56faf1f63d5b","2ecd65ea747f475c8a1cd70156c3f264","528d07b2e1f44e9583c47886b90ae4bf","dc9c769134154470b20efb875bb1565a","e40cab62f73d4a8083659ef0f9be1249","782a7e2809d3493c9dfb51dc82d23219","1ae7d7c781764a359ba5c173ab874960","b00ebb2366c44f2eb27a3f019907d18c","4178d1cab20c44fea21a39c39b577fab","dec6c39a59334c13960cea0aefc8d973","7f02472ecfe8485b9693c55ebd9409e6","c6523edabeea4af3ba1903c3eb0ff826","a95dfc1c3e76421e85c80ff8d1cc9b9f","402ee46c0dcf48138b6840930411cfc7","8a25c46487294a1684989ee5756ca768","7fea824d79a946ec995c29c4e2d30233","6b6801ccf72148e8863530b1b8dd2e8e","b2825a5216fe4001889dc9c4888bf1fd","a2234ae1a15b4242b89bfd659659171f","b0ad41f219bf4a55bbc72a7c4f09a42a","76d29377b86548c386433ab3b609cbd8","eaf13ee949e5477b918c10b32121e4e8","49f74420afee4697b7b339d10f19d342","43e7066e39e7489892991090b56e099f","2adc8ce426c3409098e91c7c4293bdc0","6b663cb4caac4a9e9691662f19b23d68","59c98a75fd3d49528db841d99153c9ac","5603890a512a4b32b16735fda84086be","c00098573f29412590bcdeb3ce8e7ef6","4f29d0e90df04d9cac143594f0edd70e","2546f98c722645aa9776e0893e5b56c9","f58591e60d8740948b59b415d8e439c7","ab91ca12557842ab89186a10e6fc4b9e","d0625cb7c4dc44799505eaa8ea5d73b8","69458abb318f4459a65f2530203e2846","c564ef370292406092713230c31f0c01","20d6e33e2f964d03a2b047b940c68f0e","4ffc3d637edd4d44aebc82d8af54efe7","674f343c27984dfbb0d1695df70de88d","4c6b495e4d2145eabbab46dbfdc5597b","e3f60b7ca0d847919efb574457a6c4e1","bbdae3b6c5404c17ac1d1ce1354a5f61","c4bd324fe395409dba28189f1fa877a2","d3525004e0914d2aad9e20597ed820da","0d5fb1acb1e346518edd8b6a86389267","d620dba2c4824460804526a211ecff47","795888bcecbf4bdc82d8a1b5b6eb317f","60d3d056ecae489dbfa6a78eadafa4bc","fea59b7aeae7482484b620beccde48b1","35cddad3225b4a329b0a86000ccfa15e","05fdc8db853244a6b81228b8d7428435","f6e7dc2c50ec4b1fb6db6f9980102125","a7aaa895453e4a07a872d980d48c9a4b","8da78ed9851d461aad3bc6b22739f382","b93a0137ebff419cb718315458a8bfa3","f51f66ba64324dffa64d483afa165029","74380a6bd1ab40d3a56fa659ab5f8f9d","f15129f80f7242b39140b8d0d7297cd3","0e6203a967b447c6ac2c94a33d6019c0","6fb999e7e60649d289728156075f43ec","2191716bfbb14507be12e3e065ddc22f","c4ca57c858ad495bad779865f196dbcf","50e7dc5fa102400880eef3f90ce9831e","39cadac49345428fb724d14b9623fba8","0701c169a9a445d9ae2315a1a96954e7","751792f24e924f6b92a1a6c774236ac8","6b3b684b579b449e93ce50d8e352d99a","60c3cab7e1ff4a048c55fce11b734498","c3d5c6b1e4d9452d9dd80231dbe011bd","8f8d1a27092e4a4288efd20d5058ab0f","651f4c952b4846278a6829e2d0f583e9","e57a529e80744cde85f91769d30f6823","8674207b663e4b73a13de3caa40edd91","460649907832416a8fdbbb76ea35b242","3ac965b43fc44523bfcb0763eec9977d","296421b82e204227bd7363f7c5382366","683b046b17ad43798acdd704691000c1","e7b0d11b74a34edabb2442a4747ea003","5ca62814f43d4113b2f3fd3bb5dec25a","d346223a98fc47dbbf5699f1c5d5a961","ec797c29ed36465a97c91ddd870d062b","3d229d64b6ef4609b0e2f92ff96fd1b3","1da621e3596f4c9fa596d80d45cb0450","1e43a2c6ea38458193e3757cae572e06","623213fec15f4382bbe40b877975f0a8"]},"id":"caHizxb9umPL","executionInfo":{"status":"ok","timestamp":1673804831309,"user_tz":0,"elapsed":2299090,"user":{"displayName":"Alvina","userId":"13758288938898952425"}},"outputId":"40b5d0d0-8fbb-4396-efd9-09012d202054"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Empty DataFrame\n","Columns: [id, text, created_at, user_id, followers_count, friends_count, favourites_count, retweet_count]\n","Index: []\n","tweet_cleaner\n","                    id                                               text  \\\n","0  1556587785007632391  RT @sophiee_ob: Customs Nike Airforce 1 Low ’J...   \n","1  1556587794582999040  Peter Obi or Peter Obi #VoteThemOut #PeterObiF...   \n","2  1556587807791087616  RT @ObidientsYouth: At the mention of OBidient...   \n","3  1556587818729742338  RT @ObidientsYouth: At the mention of OBidient...   \n","4  1556587824710819840  RT @TheArtivist_KE: Fellow Kenyans there is li...   \n","5  1556587841467064322  Nanny camera available #nairobi #nannycamera p...   \n","6  1556587942688194560  RT @AffiliateOla: I love you but your father 😂...   \n","7  1556587960929222658  A New Nigeria is POssible, #VoteThemOut #Obidi...   \n","8  1556587970693562370  RT @ObidientsYouth: At the mention of OBidient...   \n","9  1556587978067218433  RT @multimeverse: One man will come and say he...   \n","\n","                  created_at              user_id  followers_count  \\\n","0  2022-08-08 10:27:02+00:00            380215459             1760   \n","1  2022-08-08 10:27:04+00:00   994285612097261569            19172   \n","2  2022-08-08 10:27:08+00:00  1535893130599227392              227   \n","3  2022-08-08 10:27:10+00:00  1552227191605313536              278   \n","4  2022-08-08 10:27:12+00:00            374670868               44   \n","5  2022-08-08 10:27:16+00:00  1438376352428724234                5   \n","6  2022-08-08 10:27:40+00:00  1487499246429478919              144   \n","7  2022-08-08 10:27:44+00:00   994285612097261569            19172   \n","8  2022-08-08 10:27:46+00:00            532047617              470   \n","9  2022-08-08 10:27:48+00:00  1512034590038319104               56   \n","\n","   friends_count  favourites_count  retweet_count  \n","0           1625             16092             50  \n","1          16692            133015              0  \n","2            171             21342            142  \n","3           1139                18            142  \n","4            193              2181             50  \n","5             21                90              0  \n","6            272             27099             29  \n","7          16692            133015              0  \n","8            301              5568            142  \n","9            577              3660            312  \n","grammar\n","clean_text\n","grammar\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caa203f3bcae4899bd3d024a852f6ac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e15114244431409b8600173d272577c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35e4618d046449f4b70c85d636f2693e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54d9ab04375d43098cd3b9e2e3afe0ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0b1b718982b414084a01e5919b84d47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e79fc2ff46e7429db051fe3ba006374e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5778226b43634ad18c199e94a3209997"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["loaded topic model\n","0       [input_ids, attention_mask]\n","1       [input_ids, attention_mask]\n","2       [input_ids, attention_mask]\n","3       [input_ids, attention_mask]\n","4       [input_ids, attention_mask]\n","                   ...             \n","1995    [input_ids, attention_mask]\n","1996    [input_ids, attention_mask]\n","1997    [input_ids, attention_mask]\n","1998    [input_ids, attention_mask]\n","1999    [input_ids, attention_mask]\n","Name: clean_text, Length: 2000, dtype: object\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/980 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da923a7a2a0a4c48bdd5a30b3ab15b30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/540M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8deefbdccf9a4187b42ce1f3e012beca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/335 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1c4a2d6a4d94726adce67baae1057e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/843k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89d5604c5bd84a748f99ada5e6eb2874"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c39750cec0340028efd92941668c004"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1e5af84e7cd498e8433bbcce998ab1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9183f14601ee4f08a5248bcfb0c57edc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["predicted hate of tweets\n","loaded hate model\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/999 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4178d1cab20c44fea21a39c39b577fab"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/540M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0ad41f219bf4a55bbc72a7c4f09a42a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/295 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2546f98c722645aa9776e0893e5b56c9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/843k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbdae3b6c5404c17ac1d1ce1354a5f61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7aaa895453e4a07a872d980d48c9a4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39cadac49345428fb724d14b9623fba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ac965b43fc44523bfcb0763eec9977d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","got sentiment\n","got topic\n","got hate \n","got emo\n","got readability scores \n","now averaging users feeds\n","finshed averaging users feed \n","Empty DataFrame\n","Columns: [id, text, created_at, user_id, followers_count, friends_count, favourites_count, retweet_count]\n","Index: []\n","tweet_cleaner\n","                    id                                               text  \\\n","0  1556634649501188096                  Me perdí el live de jisoo y lisa.   \n","1  1556634649589616640  RT @LiliesHome_: Cute LISA, Smiling LALISA 💕\\n...   \n","2  1556634649996427265  RT @LiSA_OLiVE: 【カバヤピュアラルグミCM&amp;「#シフクノトキ」MV公...   \n","3  1556634650088448000  RT @blackpinkbabo: lisa said she had pictures ...   \n","4  1556634650331688962  RT @blackpinkbabo: Jisoo and Lisa said Jennie ...   \n","5  1556634650344554496  RT @KhmerLy2: ลูกสาวผมยาวสวยมากๆนุ่มนิ่มน่ารัก...   \n","6  1556634650721734657  RT @jenniesrenes: “li to the soo to the lisoo”...   \n","7  1556634651250524160  RT @ant210805: Góc nghiêng kìa trùi 😍\\n\\n#What...   \n","8  1556634652424675334  RT @xx_turtle_: They tried to go live as 4 but...   \n","9  1556634652550504448  RT @blckpinkpic: LISA AND JISOO https://t.co/t...   \n","\n","                  created_at              user_id  followers_count  \\\n","0  2022-08-08 13:33:15+00:00  1409969655888809985              174   \n","1  2022-08-08 13:33:15+00:00  1259515433104760832              172   \n","2  2022-08-08 13:33:16+00:00   861889837846020096             3464   \n","3  2022-08-08 13:33:16+00:00           1205108580             1227   \n","4  2022-08-08 13:33:16+00:00  1444959397562630151                7   \n","5  2022-08-08 13:33:16+00:00            149447990               83   \n","6  2022-08-08 13:33:16+00:00  1104248722286817280               52   \n","7  2022-08-08 13:33:16+00:00  1259998498050543616              174   \n","8  2022-08-08 13:33:16+00:00  1389423990175842304              237   \n","9  2022-08-08 13:33:16+00:00  1524043644780560386              119   \n","\n","   friends_count  favourites_count  retweet_count  \n","0            258              4864              0  \n","1            208             69944            490  \n","2           3255            216230            121  \n","3           5006            367476           2754  \n","4             18             17209           2138  \n","5             94               237           1743  \n","6            390             11295           4889  \n","7            323            124452              6  \n","8            220             32393           1698  \n","9            346             10085           1582  \n","grammar\n","clean_text\n","grammar\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-multi\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"celebrity_&_pop_culture\",\n","    \"3\": \"diaries_&_daily_life\",\n","    \"4\": \"family\",\n","    \"5\": \"fashion_&_style\",\n","    \"6\": \"film_tv_&_video\",\n","    \"7\": \"fitness_&_health\",\n","    \"8\": \"food_&_dining\",\n","    \"9\": \"gaming\",\n","    \"10\": \"learning_&_educational\",\n","    \"11\": \"music\",\n","    \"12\": \"news_&_social_concern\",\n","    \"13\": \"other_hobbies\",\n","    \"14\": \"relationships\",\n","    \"15\": \"science_&_technology\",\n","    \"16\": \"sports\",\n","    \"17\": \"travel_&_adventure\",\n","    \"18\": \"youth_&_student_life\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"celebrity_&_pop_culture\": 2,\n","    \"diaries_&_daily_life\": 3,\n","    \"family\": 4,\n","    \"fashion_&_style\": 5,\n","    \"film_tv_&_video\": 6,\n","    \"fitness_&_health\": 7,\n","    \"food_&_dining\": 8,\n","    \"gaming\": 9,\n","    \"learning_&_educational\": 10,\n","    \"music\": 11,\n","    \"news_&_social_concern\": 12,\n","    \"other_hobbies\": 13,\n","    \"relationships\": 14,\n","    \"science_&_technology\": 15,\n","    \"sports\": 16,\n","    \"travel_&_adventure\": 17,\n","    \"youth_&_student_life\": 18\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"hateful\",\n","    \"1\": \"targeted\",\n","    \"2\": \"aggressive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"aggressive\": 2,\n","    \"hateful\": 0,\n","    \"targeted\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["loaded topic model\n","0       [input_ids, attention_mask]\n","1       [input_ids, attention_mask]\n","2       [input_ids, attention_mask]\n","3       [input_ids, attention_mask]\n","4       [input_ids, attention_mask]\n","                   ...             \n","1995    [input_ids, attention_mask]\n","1996    [input_ids, attention_mask]\n","1997    [input_ids, attention_mask]\n","1998    [input_ids, attention_mask]\n","1999    [input_ids, attention_mask]\n","Name: clean_text, Length: 2000, dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/tokenizer_config.json\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["predicted hate of tweets\n","loaded hate model\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","got sentiment\n","got topic\n","got hate \n","got emo\n","got readability scores \n","now averaging users feeds\n","finshed averaging users feed \n","Empty DataFrame\n","Columns: [id, text, created_at, user_id, followers_count, friends_count, favourites_count, retweet_count]\n","Index: []\n","tweet_cleaner\n","                    id                                               text  \\\n","0  1556600126692548608  RT @ACORNunion: This is a crisis for our class...   \n","1  1556600130375131136  RT @ENOUGH_ND: Don’t be under any illusion, th...   \n","2  1556600168513929216  RT @Helenspicer15: @fulham_fallout @YorksBylin...   \n","3  1556600176575496192  RT @Taj_Ali1: Workers in Britain have experien...   \n","4  1556600180681621505  #EnoughIsEnough #CostOfLivingCrisis #TakeThePo...   \n","5  1556600221546823681  RT @Nurseborisbash: I've been a nurse for over...   \n","6  1556600249904513024  oh God, why palestine gonna free?\\n\\ntag::::wa...   \n","7  1556600259949854720  It is time to stand up and say #EnoughIsEnough...   \n","8  1556600278044086273  RT @PeterStefanovi2: This is absolutely superb...   \n","9  1556600294871547904  RT @tweet2chris2: EVERYONE OUT ON THE STREETS ...   \n","\n","                  created_at              user_id  followers_count  \\\n","0  2022-08-08 11:16:05+00:00            561572143              181   \n","1  2022-08-08 11:16:05+00:00            527674798             7866   \n","2  2022-08-08 11:16:15+00:00   872844815939112960              617   \n","3  2022-08-08 11:16:16+00:00            420351613             5082   \n","4  2022-08-08 11:16:17+00:00   930520648245497856             8903   \n","5  2022-08-08 11:16:27+00:00            420351613             5082   \n","6  2022-08-08 11:16:34+00:00            184461962              100   \n","7  2022-08-08 11:16:36+00:00  1162104967215362068             1667   \n","8  2022-08-08 11:16:41+00:00            103989424             1347   \n","9  2022-08-08 11:16:45+00:00            420351613             5082   \n","\n","   friends_count  favourites_count  retweet_count  \n","0            485             15295             51  \n","1           7336            159930             47  \n","2            619             32418             56  \n","3           3792            236980            297  \n","4           9772            196189              2  \n","5           3792            236980            159  \n","6            149                75              0  \n","7           1759              2050              0  \n","8           1382            228092           1169  \n","9           3792            236980             59  \n","grammar\n","clean_text\n","grammar\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-multi\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"celebrity_&_pop_culture\",\n","    \"3\": \"diaries_&_daily_life\",\n","    \"4\": \"family\",\n","    \"5\": \"fashion_&_style\",\n","    \"6\": \"film_tv_&_video\",\n","    \"7\": \"fitness_&_health\",\n","    \"8\": \"food_&_dining\",\n","    \"9\": \"gaming\",\n","    \"10\": \"learning_&_educational\",\n","    \"11\": \"music\",\n","    \"12\": \"news_&_social_concern\",\n","    \"13\": \"other_hobbies\",\n","    \"14\": \"relationships\",\n","    \"15\": \"science_&_technology\",\n","    \"16\": \"sports\",\n","    \"17\": \"travel_&_adventure\",\n","    \"18\": \"youth_&_student_life\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"celebrity_&_pop_culture\": 2,\n","    \"diaries_&_daily_life\": 3,\n","    \"family\": 4,\n","    \"fashion_&_style\": 5,\n","    \"film_tv_&_video\": 6,\n","    \"fitness_&_health\": 7,\n","    \"food_&_dining\": 8,\n","    \"gaming\": 9,\n","    \"learning_&_educational\": 10,\n","    \"music\": 11,\n","    \"news_&_social_concern\": 12,\n","    \"other_hobbies\": 13,\n","    \"relationships\": 14,\n","    \"science_&_technology\": 15,\n","    \"sports\": 16,\n","    \"travel_&_adventure\": 17,\n","    \"youth_&_student_life\": 18\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"hateful\",\n","    \"1\": \"targeted\",\n","    \"2\": \"aggressive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"aggressive\": 2,\n","    \"hateful\": 0,\n","    \"targeted\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["loaded topic model\n","0       [input_ids, attention_mask]\n","1       [input_ids, attention_mask]\n","2       [input_ids, attention_mask]\n","3       [input_ids, attention_mask]\n","4       [input_ids, attention_mask]\n","                   ...             \n","1995    [input_ids, attention_mask]\n","1996    [input_ids, attention_mask]\n","1997    [input_ids, attention_mask]\n","1998    [input_ids, attention_mask]\n","1999    [input_ids, attention_mask]\n","Name: clean_text, Length: 2000, dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/tokenizer_config.json\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["predicted hate of tweets\n","loaded hate model\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","got sentiment\n","got topic\n","got hate \n","got emo\n","got readability scores \n","now averaging users feeds\n","finshed averaging users feed \n","Empty DataFrame\n","Columns: [id, text, created_at, user_id, followers_count, friends_count, favourites_count, retweet_count]\n","Index: []\n","tweet_cleaner\n","                    id                                               text  \\\n","0  1553394373601148928  @Sen_JoeManchin High #energyprices are likely ...   \n","1  1553396201197912064  @Sen_JoeManchin High #energyprices are likely ...   \n","2  1553397986599788545  A large factor playing a role in the #UnitedSt...   \n","3  1553410577682370561  I just signed the open letter, calling for Cen...   \n","4  1553446650915553283  https://t.co/RZR7rsKz7m\\n#BorisJohnson #energy...   \n","5  1553459606600536064  RT @capitolreport: A large factor playing a ro...   \n","6  1553469704345460738  If we were living in olden times and we got ou...   \n","7  1553486045156884480  @katiebecker16 @LizSward Just to clarify- the ...   \n","8  1553494666703540224  Estimados europeos y occidente en general: he ...   \n","9  1553495363348201472  RT @AndreaIniguezM: Estimados europeos y occid...   \n","\n","                  created_at              user_id  followers_count  \\\n","0  2022-07-30 14:57:33+00:00            317457162               19   \n","1  2022-07-30 15:04:49+00:00            317457162               19   \n","2  2022-07-30 15:11:55+00:00  1443638153722671112              498   \n","3  2022-07-30 16:01:57+00:00           1546212133              266   \n","4  2022-07-30 18:25:17+00:00            117798721              263   \n","5  2022-07-30 19:16:46+00:00  1366433581749436423             3540   \n","6  2022-07-30 19:56:54+00:00  1444752807849041920              721   \n","7  2022-07-30 21:01:50+00:00            154710967               81   \n","8  2022-07-30 21:36:05+00:00  1016463545989566475             4289   \n","9  2022-07-30 21:38:51+00:00  1370477580030259204             8159   \n","\n","   friends_count  favourites_count  retweet_count  \n","0              0                11              0  \n","1              0                11              0  \n","2             19               970              1  \n","3           1443              9137              0  \n","4            110            100084              0  \n","5            628               634              1  \n","6            847              8836              0  \n","7            443               910              0  \n","8           1114             47442              9  \n","9           8677             30801              9  \n","grammar\n","clean_text\n","grammar\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-multi\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"celebrity_&_pop_culture\",\n","    \"3\": \"diaries_&_daily_life\",\n","    \"4\": \"family\",\n","    \"5\": \"fashion_&_style\",\n","    \"6\": \"film_tv_&_video\",\n","    \"7\": \"fitness_&_health\",\n","    \"8\": \"food_&_dining\",\n","    \"9\": \"gaming\",\n","    \"10\": \"learning_&_educational\",\n","    \"11\": \"music\",\n","    \"12\": \"news_&_social_concern\",\n","    \"13\": \"other_hobbies\",\n","    \"14\": \"relationships\",\n","    \"15\": \"science_&_technology\",\n","    \"16\": \"sports\",\n","    \"17\": \"travel_&_adventure\",\n","    \"18\": \"youth_&_student_life\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"celebrity_&_pop_culture\": 2,\n","    \"diaries_&_daily_life\": 3,\n","    \"family\": 4,\n","    \"fashion_&_style\": 5,\n","    \"film_tv_&_video\": 6,\n","    \"fitness_&_health\": 7,\n","    \"food_&_dining\": 8,\n","    \"gaming\": 9,\n","    \"learning_&_educational\": 10,\n","    \"music\": 11,\n","    \"news_&_social_concern\": 12,\n","    \"other_hobbies\": 13,\n","    \"relationships\": 14,\n","    \"science_&_technology\": 15,\n","    \"sports\": 16,\n","    \"travel_&_adventure\": 17,\n","    \"youth_&_student_life\": 18\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"hateful\",\n","    \"1\": \"targeted\",\n","    \"2\": \"aggressive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"aggressive\": 2,\n","    \"hateful\": 0,\n","    \"targeted\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["loaded topic model\n","0       [input_ids, attention_mask]\n","1       [input_ids, attention_mask]\n","2       [input_ids, attention_mask]\n","3       [input_ids, attention_mask]\n","4       [input_ids, attention_mask]\n","                   ...             \n","6182    [input_ids, attention_mask]\n","6183    [input_ids, attention_mask]\n","6184    [input_ids, attention_mask]\n","6185    [input_ids, attention_mask]\n","6186    [input_ids, attention_mask]\n","Name: clean_text, Length: 6187, dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/tokenizer_config.json\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["predicted hate of tweets\n","loaded hate model\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","got sentiment\n","got topic\n","got hate \n","got emo\n","got readability scores \n","now averaging users feeds\n","finshed averaging users feed \n","Empty DataFrame\n","Columns: [id, text, created_at, user_id, followers_count, friends_count, favourites_count, retweet_count]\n","Index: []\n","tweet_cleaner\n","                    id                                               text  \\\n","0  1553724402193383424                           I'm not ready for #iOS16   \n","1  1553724495810433024  RT @lshlj: #iOS16 혁신: 드디어 ㄷ을 3번 누르면 ㄸㄷ가 아니라 ㄷㄷ...   \n","2  1553733336027242496  RT @saradietschy: Desk view is honestly kinda ...   \n","3  1553735058346758144  #Android #iPhone #iOS16 #iphone14 #apple https...   \n","4  1553737307550777345  RT @JacheMoon: ConceptX16 : Twitter (2 Styles)...   \n","5  1553737731389493248  What's #Best on https://t.co/flXwnOkooM ?\\nNew...   \n","6  1553737826734424069  #Best of the day on https://t.co/k05yokndMa\\nN...   \n","7  1553737831406866432  Download the Best #app to share your #Best #te...   \n","8  1553737846695292929  RT @BestTLD: Download the Best #app to share y...   \n","9  1553737851195789312  RT @BestTLD: Download the Best #app to share y...   \n","\n","                  created_at              user_id  followers_count  \\\n","0  2022-07-31 12:48:58+00:00   795990463190859776               36   \n","1  2022-07-31 12:49:21+00:00   823923913273016320              357   \n","2  2022-07-31 13:24:28+00:00  1322262194592141313               53   \n","3  2022-07-31 13:31:19+00:00  1154028970720952320                7   \n","4  2022-07-31 13:40:15+00:00  1550588274342854661               87   \n","5  2022-07-31 13:41:56+00:00           2338348428             2220   \n","6  2022-07-31 13:42:19+00:00           2300501173             3259   \n","7  2022-07-31 13:42:20+00:00           2300501173             3259   \n","8  2022-07-31 13:42:24+00:00  1142424032794406912            29958   \n","9  2022-07-31 13:42:25+00:00   710123736175783938            41816   \n","\n","   friends_count  favourites_count  retweet_count  \n","0            412              2350              0  \n","1           3108              7498           1868  \n","2           1036              3737           5670  \n","3             31              1726              0  \n","4            230                40              3  \n","5           4385              3080              0  \n","6           4519             82350              0  \n","7           4519             82350              2  \n","8              2                 0              2  \n","9              1                 7              2  \n","grammar\n","clean_text\n","grammar\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-multi\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"celebrity_&_pop_culture\",\n","    \"3\": \"diaries_&_daily_life\",\n","    \"4\": \"family\",\n","    \"5\": \"fashion_&_style\",\n","    \"6\": \"film_tv_&_video\",\n","    \"7\": \"fitness_&_health\",\n","    \"8\": \"food_&_dining\",\n","    \"9\": \"gaming\",\n","    \"10\": \"learning_&_educational\",\n","    \"11\": \"music\",\n","    \"12\": \"news_&_social_concern\",\n","    \"13\": \"other_hobbies\",\n","    \"14\": \"relationships\",\n","    \"15\": \"science_&_technology\",\n","    \"16\": \"sports\",\n","    \"17\": \"travel_&_adventure\",\n","    \"18\": \"youth_&_student_life\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"celebrity_&_pop_culture\": 2,\n","    \"diaries_&_daily_life\": 3,\n","    \"family\": 4,\n","    \"fashion_&_style\": 5,\n","    \"film_tv_&_video\": 6,\n","    \"fitness_&_health\": 7,\n","    \"food_&_dining\": 8,\n","    \"gaming\": 9,\n","    \"learning_&_educational\": 10,\n","    \"music\": 11,\n","    \"news_&_social_concern\": 12,\n","    \"other_hobbies\": 13,\n","    \"relationships\": 14,\n","    \"science_&_technology\": 15,\n","    \"sports\": 16,\n","    \"travel_&_adventure\": 17,\n","    \"youth_&_student_life\": 18\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"hateful\",\n","    \"1\": \"targeted\",\n","    \"2\": \"aggressive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"aggressive\": 2,\n","    \"hateful\": 0,\n","    \"targeted\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["loaded topic model\n","0       [input_ids, attention_mask]\n","1       [input_ids, attention_mask]\n","2       [input_ids, attention_mask]\n","3       [input_ids, attention_mask]\n","4       [input_ids, attention_mask]\n","                   ...             \n","4995    [input_ids, attention_mask]\n","4996    [input_ids, attention_mask]\n","4997    [input_ids, attention_mask]\n","4998    [input_ids, attention_mask]\n","4999    [input_ids, attention_mask]\n","Name: clean_text, Length: 5000, dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/tokenizer_config.json\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["predicted hate of tweets\n","loaded hate model\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","got sentiment\n","got topic\n","got hate \n","got emo\n","got readability scores \n","now averaging users feeds\n","finshed averaging users feed \n","Empty DataFrame\n","Columns: [id, text, created_at, user_id, followers_count, friends_count, favourites_count, retweet_count]\n","Index: []\n","tweet_cleaner\n","                    id                                               text  \\\n","0  1556541612821553152  #FLASHINFO \\nLe ministère de la Défense de #Ta...   \n","1  1556541624951472130  RT @MOFA_Taiwan: Minister Wu extended a warm w...   \n","2  1556541635386904576  RT @LouisBoyart: Je suis un enfant #HP (Haut P...   \n","3  1556541639744790529  RT @Zhou_Li_CHN: \"#Türkiye is firmly committed...   \n","4  1556541640310788096  RT @NFSC_HAGnews: Large fleet of fishing boats...   \n","5  1556541647139115008  RT @Wenyin2064: Chinese Ambassador said to re-...   \n","6  1556541653325864960  RT @ChineseEmbinUK: .@AmbZhengZeguang lodged s...   \n","7  1556541687811506178  RT @Therealgsns: BREAKING! Top Taiwan defence ...   \n","8  1556541691091369987  RT @Fallen_x_King: Scenes:\\n#WWIII #Taiwan #WW...   \n","9  1556541699689574400  RT @IndoPac_Info: A Taiwanese Fighter Jet cond...   \n","\n","                  created_at              user_id  followers_count  \\\n","0  2022-08-08 07:23:34+00:00  1273218547846729729            53609   \n","1  2022-08-08 07:23:37+00:00           2999543756             1240   \n","2  2022-08-08 07:23:39+00:00  1430899835272454150             2651   \n","3  2022-08-08 07:23:40+00:00           2909445183             1800   \n","4  2022-08-08 07:23:40+00:00  1493122848851910659              905   \n","5  2022-08-08 07:23:42+00:00  1493122848851910659              905   \n","6  2022-08-08 07:23:43+00:00           2720051267              572   \n","7  2022-08-08 07:23:52+00:00             67839403              335   \n","8  2022-08-08 07:23:52+00:00  1485298277679517699                6   \n","9  2022-08-08 07:23:55+00:00  1167660096060395521             1243   \n","\n","   friends_count  favourites_count  retweet_count  \n","0             78                73              0  \n","1            899             19514            212  \n","2           3132             28928             68  \n","3            850             24849             26  \n","4            331                39             41  \n","5            331                39              2  \n","6            517            272552              5  \n","7            507             20223              3  \n","8             18               281           6510  \n","9           1248             80393             56  \n","grammar\n","clean_text\n","grammar\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-multi\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"celebrity_&_pop_culture\",\n","    \"3\": \"diaries_&_daily_life\",\n","    \"4\": \"family\",\n","    \"5\": \"fashion_&_style\",\n","    \"6\": \"film_tv_&_video\",\n","    \"7\": \"fitness_&_health\",\n","    \"8\": \"food_&_dining\",\n","    \"9\": \"gaming\",\n","    \"10\": \"learning_&_educational\",\n","    \"11\": \"music\",\n","    \"12\": \"news_&_social_concern\",\n","    \"13\": \"other_hobbies\",\n","    \"14\": \"relationships\",\n","    \"15\": \"science_&_technology\",\n","    \"16\": \"sports\",\n","    \"17\": \"travel_&_adventure\",\n","    \"18\": \"youth_&_student_life\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"celebrity_&_pop_culture\": 2,\n","    \"diaries_&_daily_life\": 3,\n","    \"family\": 4,\n","    \"fashion_&_style\": 5,\n","    \"film_tv_&_video\": 6,\n","    \"fitness_&_health\": 7,\n","    \"food_&_dining\": 8,\n","    \"gaming\": 9,\n","    \"learning_&_educational\": 10,\n","    \"music\": 11,\n","    \"news_&_social_concern\": 12,\n","    \"other_hobbies\": 13,\n","    \"relationships\": 14,\n","    \"science_&_technology\": 15,\n","    \"sports\": 16,\n","    \"travel_&_adventure\": 17,\n","    \"youth_&_student_life\": 18\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"hateful\",\n","    \"1\": \"targeted\",\n","    \"2\": \"aggressive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"aggressive\": 2,\n","    \"hateful\": 0,\n","    \"targeted\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["loaded topic model\n","0       [input_ids, attention_mask]\n","1       [input_ids, attention_mask]\n","2       [input_ids, attention_mask]\n","3       [input_ids, attention_mask]\n","4       [input_ids, attention_mask]\n","                   ...             \n","9995    [input_ids, attention_mask]\n","9996    [input_ids, attention_mask]\n","9997    [input_ids, attention_mask]\n","9998    [input_ids, attention_mask]\n","9999    [input_ids, attention_mask]\n","Name: clean_text, Length: 10000, dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/tokenizer_config.json\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["predicted hate of tweets\n","loaded hate model\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","got sentiment\n","got topic\n","got hate \n","got emo\n","got readability scores \n","now averaging users feeds\n","finshed averaging users feed \n","Empty DataFrame\n","Columns: [id, text, created_at, user_id, followers_count, friends_count, favourites_count, retweet_count]\n","Index: []\n","tweet_cleaner\n","                    id                                               text  \\\n","0  1556759148565594113  RT @Freezer3emeForm: #ONEPIECE #ONEPIECE1056 #...   \n","1  1556759159408054272  RT @jump_henshubu: ／\\nフォロー＆RTで当たる🎯\\n＼\\n劇場版「FIL...   \n","2  1556759164478791682  RT @OPBR_official: 📢さらに‼️\\n\\nみんなで獲得した合計ポイントが15...   \n","3  1556759175551524864  RT @OPBR_official: 📢さらに‼️\\n\\nみんなで獲得した合計ポイントが15...   \n","4  1556759179473428480  RT @oekakiboya: ナミのげんこつには敵わない\\n#ONEPIECE https...   \n","5  1556759182186930178  RT @OPBR_official: 🔴#トレジャークルーズ ✖️ #バウンティラッシュ\\n...   \n","6  1556759187572785152  RT @OPBR_official: 🎶#夏の海賊ダブルフェス \\nTwitterキャンペー...   \n","7  1556759190626250752  RT @Wantsy_M: ^q^ #ONEPIECE #ゾロサン https://t.co...   \n","8  1556759192131739648  RT @oekakiboya: ナミのげんこつには敵わない\\n#ONEPIECE https...   \n","9  1556759197781475335  RT @OPCrew_Oficial: Comparación del anime (101...   \n","\n","                  created_at              user_id  followers_count  \\\n","0  2022-08-08 21:47:58+00:00  1358800436585766917                1   \n","1  2022-08-08 21:48:01+00:00  1128786212779466752               26   \n","2  2022-08-08 21:48:02+00:00  1418248136527687684                0   \n","3  2022-08-08 21:48:05+00:00  1154253873508282369               83   \n","4  2022-08-08 21:48:06+00:00  1246517060601069569              269   \n","5  2022-08-08 21:48:06+00:00  1468534084251299845                4   \n","6  2022-08-08 21:48:08+00:00  1473427956609744896                0   \n","7  2022-08-08 21:48:08+00:00  1394287365800136707               77   \n","8  2022-08-08 21:48:09+00:00  1099888270303993856              145   \n","9  2022-08-08 21:48:10+00:00            752988362               72   \n","\n","   friends_count  favourites_count  retweet_count  \n","0             59               414           2115  \n","1            122               260          20474  \n","2              3                47           9152  \n","3            262              5358           9152  \n","4            452             79390           1217  \n","5             19               102           9335  \n","6              6                 7          26486  \n","7           1847              9358            869  \n","8            188             14475           1217  \n","9             68             13935            338  \n","grammar\n","clean_text\n","grammar\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-multi\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"celebrity_&_pop_culture\",\n","    \"3\": \"diaries_&_daily_life\",\n","    \"4\": \"family\",\n","    \"5\": \"fashion_&_style\",\n","    \"6\": \"film_tv_&_video\",\n","    \"7\": \"fitness_&_health\",\n","    \"8\": \"food_&_dining\",\n","    \"9\": \"gaming\",\n","    \"10\": \"learning_&_educational\",\n","    \"11\": \"music\",\n","    \"12\": \"news_&_social_concern\",\n","    \"13\": \"other_hobbies\",\n","    \"14\": \"relationships\",\n","    \"15\": \"science_&_technology\",\n","    \"16\": \"sports\",\n","    \"17\": \"travel_&_adventure\",\n","    \"18\": \"youth_&_student_life\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"celebrity_&_pop_culture\": 2,\n","    \"diaries_&_daily_life\": 3,\n","    \"family\": 4,\n","    \"fashion_&_style\": 5,\n","    \"film_tv_&_video\": 6,\n","    \"fitness_&_health\": 7,\n","    \"food_&_dining\": 8,\n","    \"gaming\": 9,\n","    \"learning_&_educational\": 10,\n","    \"music\": 11,\n","    \"news_&_social_concern\": 12,\n","    \"other_hobbies\": 13,\n","    \"relationships\": 14,\n","    \"science_&_technology\": 15,\n","    \"sports\": 16,\n","    \"travel_&_adventure\": 17,\n","    \"youth_&_student_life\": 18\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"hateful\",\n","    \"1\": \"targeted\",\n","    \"2\": \"aggressive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"aggressive\": 2,\n","    \"hateful\": 0,\n","    \"targeted\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["loaded topic model\n","0       [input_ids, attention_mask]\n","1       [input_ids, attention_mask]\n","2       [input_ids, attention_mask]\n","3       [input_ids, attention_mask]\n","4       [input_ids, attention_mask]\n","                   ...             \n","4995    [input_ids, attention_mask]\n","4996    [input_ids, attention_mask]\n","4997    [input_ids, attention_mask]\n","4998    [input_ids, attention_mask]\n","4999    [input_ids, attention_mask]\n","Name: clean_text, Length: 5000, dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/tokenizer_config.json\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["predicted hate of tweets\n","loaded hate model\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","got sentiment\n","got topic\n","got hate \n","got emo\n","got readability scores \n","now averaging users feeds\n","finshed averaging users feed \n","Empty DataFrame\n","Columns: [id, text, created_at, user_id, followers_count, friends_count, favourites_count, retweet_count]\n","Index: []\n","tweet_cleaner\n","                    id                                               text  \\\n","0  1556746424242552832  RT @BylinesScotland: When a former Prime Minis...   \n","1  1556746446313078785  RT @Jennife87220510: It’s not a #CostOfLivingC...   \n","2  1556746454953299968  RT @Artboy1: Let's make sure that this message...   \n","3  1556746457931304960  RT @agcolehamilton: The right call, we need th...   \n","4  1556746480253280259  RT @AllisonPearson: I am so angry at the pious...   \n","5  1556746487542980608  RT @SlothBracelet: Brandon Lewis won't have an...   \n","6  1556746502487379972  RT @AllisonPearson: I am so angry at the pious...   \n","7  1556746502793580546  RT @AllisonPearson: I am so angry at the pious...   \n","8  1556746504362131466  RT @AllisonPearson: I am so angry at the pious...   \n","9  1556746530354233345  RT @ajrob41: I have no confidence in the #Cons...   \n","\n","                  created_at              user_id  followers_count  \\\n","0  2022-08-08 20:57:25+00:00           2654704009             1099   \n","1  2022-08-08 20:57:30+00:00            100454091             1589   \n","2  2022-08-08 20:57:32+00:00  1150384291706261504             4456   \n","3  2022-08-08 20:57:33+00:00             83446974              462   \n","4  2022-08-08 20:57:38+00:00  1540771851709739010             1321   \n","5  2022-08-08 20:57:40+00:00  1254366076055564288             4277   \n","6  2022-08-08 20:57:43+00:00           3423801141              254   \n","7  2022-08-08 20:57:43+00:00  1518688770106740737             1567   \n","8  2022-08-08 20:57:44+00:00  1521152164613218305               57   \n","9  2022-08-08 20:57:50+00:00           1505791153             2657   \n","\n","   friends_count  favourites_count  retweet_count  \n","0           1494             73986             88  \n","1           2055             86116             65  \n","2           4989             16758             48  \n","3           1243             20829              6  \n","4            745               205            234  \n","5           4979             84898            103  \n","6            249             57355            234  \n","7           4036              4407            234  \n","8            560              2931            234  \n","9           4183            177321            146  \n","grammar\n","clean_text\n","grammar\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-multi\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"celebrity_&_pop_culture\",\n","    \"3\": \"diaries_&_daily_life\",\n","    \"4\": \"family\",\n","    \"5\": \"fashion_&_style\",\n","    \"6\": \"film_tv_&_video\",\n","    \"7\": \"fitness_&_health\",\n","    \"8\": \"food_&_dining\",\n","    \"9\": \"gaming\",\n","    \"10\": \"learning_&_educational\",\n","    \"11\": \"music\",\n","    \"12\": \"news_&_social_concern\",\n","    \"13\": \"other_hobbies\",\n","    \"14\": \"relationships\",\n","    \"15\": \"science_&_technology\",\n","    \"16\": \"sports\",\n","    \"17\": \"travel_&_adventure\",\n","    \"18\": \"youth_&_student_life\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"celebrity_&_pop_culture\": 2,\n","    \"diaries_&_daily_life\": 3,\n","    \"family\": 4,\n","    \"fashion_&_style\": 5,\n","    \"film_tv_&_video\": 6,\n","    \"fitness_&_health\": 7,\n","    \"food_&_dining\": 8,\n","    \"gaming\": 9,\n","    \"learning_&_educational\": 10,\n","    \"music\": 11,\n","    \"news_&_social_concern\": 12,\n","    \"other_hobbies\": 13,\n","    \"relationships\": 14,\n","    \"science_&_technology\": 15,\n","    \"sports\": 16,\n","    \"travel_&_adventure\": 17,\n","    \"youth_&_student_life\": 18\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"hateful\",\n","    \"1\": \"targeted\",\n","    \"2\": \"aggressive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"aggressive\": 2,\n","    \"hateful\": 0,\n","    \"targeted\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["loaded topic model\n","0       [input_ids, attention_mask]\n","1       [input_ids, attention_mask]\n","2       [input_ids, attention_mask]\n","3       [input_ids, attention_mask]\n","4       [input_ids, attention_mask]\n","                   ...             \n","4995    [input_ids, attention_mask]\n","4996    [input_ids, attention_mask]\n","4997    [input_ids, attention_mask]\n","4998    [input_ids, attention_mask]\n","4999    [input_ids, attention_mask]\n","Name: clean_text, Length: 5000, dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/tokenizer_config.json\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["predicted hate of tweets\n","loaded hate model\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","got sentiment\n","got topic\n","got hate \n","got emo\n","got readability scores \n","now averaging users feeds\n","finshed averaging users feed \n","Empty DataFrame\n","Columns: [id, text, created_at, user_id, followers_count, friends_count, favourites_count, retweet_count]\n","Index: []\n","tweet_cleaner\n","                    id                                               text  \\\n","0  1559281657584164867  RT @MrJingle14: #GetBackToWorkYouFatPonce is t...   \n","1  1559281660566413314  RT @DrSyn4: #Getbacktoworkyoufatponce https://...   \n","2  1559281678098595843  RT @B__50000_: Loving this hashtag 👇 \\n\\n#GetB...   \n","3  1559281679080071169  RT @tonyrawdin9: Has he ever done a day’s work...   \n","4  1559281679486930949  RT @Parody_PM: Today, I am mostly being a lazy...   \n","5  1559281679730188288  RT @SallyMi83941850: In a G7 nation. A country...   \n","6  1559281687112073216  #Getbacktoworkyoufatponce https://t.co/UUeuIBkBaI   \n","7  1559281687737032705  RT @LincsLimpet: @AnthonyCockbur2 @LGalloway77...   \n","8  1559281690098520065  RT @THemingford: Seriously - RT this if you do...   \n","9  1559281698780725248  RT @i_nautilus: Has there ever been a Prime Mi...   \n","\n","                  created_at              user_id  followers_count  \\\n","0  2022-08-15 20:51:31+00:00   715285822598168577             4102   \n","1  2022-08-15 20:51:32+00:00           2663662412               34   \n","2  2022-08-15 20:51:36+00:00            321340097              551   \n","3  2022-08-15 20:51:37+00:00   718856899429576705             1458   \n","4  2022-08-15 20:51:37+00:00           4201996751             5502   \n","5  2022-08-15 20:51:37+00:00  1450080711826935816             5077   \n","6  2022-08-15 20:51:38+00:00            582538139              472   \n","7  2022-08-15 20:51:39+00:00   715285822598168577             4102   \n","8  2022-08-15 20:51:39+00:00  1256511141586644992             1076   \n","9  2022-08-15 20:51:41+00:00            121517017              962   \n","\n","   friends_count  favourites_count  retweet_count  \n","0           5001             85530             62  \n","1             59              8417            448  \n","2           1217             53526            378  \n","3           1735             35414            422  \n","4           5446            315451           1381  \n","5           5558              5540             29  \n","6           1350             98705             49  \n","7           5001             85530             22  \n","8           1516             51476           1170  \n","9           2215              2698            105  \n","grammar\n","clean_text\n","grammar\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-multi\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"celebrity_&_pop_culture\",\n","    \"3\": \"diaries_&_daily_life\",\n","    \"4\": \"family\",\n","    \"5\": \"fashion_&_style\",\n","    \"6\": \"film_tv_&_video\",\n","    \"7\": \"fitness_&_health\",\n","    \"8\": \"food_&_dining\",\n","    \"9\": \"gaming\",\n","    \"10\": \"learning_&_educational\",\n","    \"11\": \"music\",\n","    \"12\": \"news_&_social_concern\",\n","    \"13\": \"other_hobbies\",\n","    \"14\": \"relationships\",\n","    \"15\": \"science_&_technology\",\n","    \"16\": \"sports\",\n","    \"17\": \"travel_&_adventure\",\n","    \"18\": \"youth_&_student_life\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"celebrity_&_pop_culture\": 2,\n","    \"diaries_&_daily_life\": 3,\n","    \"family\": 4,\n","    \"fashion_&_style\": 5,\n","    \"film_tv_&_video\": 6,\n","    \"fitness_&_health\": 7,\n","    \"food_&_dining\": 8,\n","    \"gaming\": 9,\n","    \"learning_&_educational\": 10,\n","    \"music\": 11,\n","    \"news_&_social_concern\": 12,\n","    \"other_hobbies\": 13,\n","    \"relationships\": 14,\n","    \"science_&_technology\": 15,\n","    \"sports\": 16,\n","    \"travel_&_adventure\": 17,\n","    \"youth_&_student_life\": 18\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"hateful\",\n","    \"1\": \"targeted\",\n","    \"2\": \"aggressive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"aggressive\": 2,\n","    \"hateful\": 0,\n","    \"targeted\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["loaded topic model\n","0       [input_ids, attention_mask]\n","1       [input_ids, attention_mask]\n","2       [input_ids, attention_mask]\n","3       [input_ids, attention_mask]\n","4       [input_ids, attention_mask]\n","                   ...             \n","4995    [input_ids, attention_mask]\n","4996    [input_ids, attention_mask]\n","4997    [input_ids, attention_mask]\n","4998    [input_ids, attention_mask]\n","4999    [input_ids, attention_mask]\n","Name: clean_text, Length: 5000, dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/tokenizer_config.json\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted hate of tweets\n","loaded hate model\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","got sentiment\n","got topic\n","got hate \n","got emo\n","got readability scores \n","now averaging users feeds\n","finshed averaging users feed \n","Empty DataFrame\n","Columns: [id, text, created_at, user_id, followers_count, friends_count, favourites_count, retweet_count]\n","Index: []\n","tweet_cleaner\n","                    id                                               text  \\\n","0  1553430755128254465  Comienzo con una conferencia gratuita el próxi...   \n","1  1553442916500127746  RT @MarlynCamachoR: El próximo #BookLoversDay ...   \n","2  1553494059532042240  RT @MarlynCamachoR: El próximo #BookLoversDay ...   \n","3  1553813912520384512  A big day in the book world is coming up, Nati...   \n","4  1553815007871262721  RT @FitgersBooks: A big day in the book world ...   \n","5  1553834192739475457  Promote your #book in time!\\nINTERNATIONAL BOO...   \n","6  1554124603223846912  RT @GabrielBlake_: Try these spine-chilling bo...   \n","7  1554200390974980099  Experiencing the world through Chris Maltby’s ...   \n","8  1554207259655393288  Naughty Nata \\nby Donna Deal \\n\\n⭐️⭐️⭐️⭐️⭐️ RA...   \n","9  1554332322027405312  RT @kingsenglishdpt: As it is #BookLoversDay i...   \n","\n","                  created_at             user_id  followers_count  \\\n","0  2022-07-30 17:22:08+00:00           146618784               20   \n","1  2022-07-30 18:10:27+00:00           321482594              290   \n","2  2022-07-30 21:33:40+00:00           146952830              400   \n","3  2022-07-31 18:44:39+00:00          1957570860             1379   \n","4  2022-07-31 18:49:01+00:00           244964328             2472   \n","5  2022-07-31 20:05:15+00:00          2892463432            19407   \n","6  2022-08-01 15:19:14+00:00  893460901470154752            10267   \n","7  2022-08-01 20:20:23+00:00          2892463432            19407   \n","8  2022-08-01 20:47:41+00:00          2892463432            19407   \n","9  2022-08-02 05:04:38+00:00          2744039040              768   \n","\n","   friends_count  favourites_count  retweet_count  \n","0             17             15199              0  \n","1           1174               385              2  \n","2            790             28648              2  \n","3           3375              4067              2  \n","4           2264            190459              2  \n","5          20270              3626              1  \n","6           8753             75672            475  \n","7          20270              3626              0  \n","8          20270              3626              0  \n","9           5002            185474              1  \n","grammar\n","clean_text\n","grammar\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-multi\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"celebrity_&_pop_culture\",\n","    \"3\": \"diaries_&_daily_life\",\n","    \"4\": \"family\",\n","    \"5\": \"fashion_&_style\",\n","    \"6\": \"film_tv_&_video\",\n","    \"7\": \"fitness_&_health\",\n","    \"8\": \"food_&_dining\",\n","    \"9\": \"gaming\",\n","    \"10\": \"learning_&_educational\",\n","    \"11\": \"music\",\n","    \"12\": \"news_&_social_concern\",\n","    \"13\": \"other_hobbies\",\n","    \"14\": \"relationships\",\n","    \"15\": \"science_&_technology\",\n","    \"16\": \"sports\",\n","    \"17\": \"travel_&_adventure\",\n","    \"18\": \"youth_&_student_life\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"celebrity_&_pop_culture\": 2,\n","    \"diaries_&_daily_life\": 3,\n","    \"family\": 4,\n","    \"fashion_&_style\": 5,\n","    \"film_tv_&_video\": 6,\n","    \"fitness_&_health\": 7,\n","    \"food_&_dining\": 8,\n","    \"gaming\": 9,\n","    \"learning_&_educational\": 10,\n","    \"music\": 11,\n","    \"news_&_social_concern\": 12,\n","    \"other_hobbies\": 13,\n","    \"relationships\": 14,\n","    \"science_&_technology\": 15,\n","    \"sports\": 16,\n","    \"travel_&_adventure\": 17,\n","    \"youth_&_student_life\": 18\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"hateful\",\n","    \"1\": \"targeted\",\n","    \"2\": \"aggressive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"aggressive\": 2,\n","    \"hateful\": 0,\n","    \"targeted\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["loaded topic model\n","0       [input_ids, attention_mask]\n","1       [input_ids, attention_mask]\n","2       [input_ids, attention_mask]\n","3       [input_ids, attention_mask]\n","4       [input_ids, attention_mask]\n","                   ...             \n","2444    [input_ids, attention_mask]\n","2445    [input_ids, attention_mask]\n","2446    [input_ids, attention_mask]\n","2447    [input_ids, attention_mask]\n","2448    [input_ids, attention_mask]\n","Name: clean_text, Length: 2449, dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/tokenizer_config.json\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["predicted hate of tweets\n","loaded hate model\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","got sentiment\n","got topic\n","got hate \n","got emo\n","got readability scores \n","now averaging users feeds\n","finshed averaging users feed \n","Empty DataFrame\n","Columns: [id, text, created_at, user_id, followers_count, friends_count, favourites_count, retweet_count]\n","Index: []\n","tweet_cleaner\n","                    id                                               text  \\\n","0  1559034163583504387  Anne Heche, 'Wag the Dog' and 'Donnie Brasco' ...   \n","1  1559034163604381696  Analysis: Salman Rushdie interviewer suggests ...   \n","2  1559034163625447425  Salman Rushdie is recovering from 'life-changi...   \n","3  1559034244932018179  RT @ChuckDeVore: @elonmusk @BillFOXLA The #Bid...   \n","4  1559034330759962624  RT @IronDonkey556: Fuck this guy and his entir...   \n","5  1559034481503309827  RT @LepretreTony: Une seconde provocation pour...   \n","6  1559034570183516160  RT @Islampaal_1: #د_اسد۲۴مه \\n#۲۴_اسد\\nعبدالقه...   \n","7  1559034716128481280  OFERTA! SOLO POR POCOS DIAS-&gt; https://t.co/...   \n","8  1559034734516531200  RT @kk131066: Those who don't have the guts to...   \n","9  1559034753491369984  RT @TamieUSCongress: I spoke with @POTUS today...   \n","\n","                  created_at              user_id  followers_count  \\\n","0  2022-08-15 04:28:04+00:00  1167202523330969601            13834   \n","1  2022-08-15 04:28:04+00:00  1167202523330969601            13834   \n","2  2022-08-15 04:28:04+00:00  1167202523330969601            13834   \n","3  2022-08-15 04:28:24+00:00            105011437             3485   \n","4  2022-08-15 04:28:44+00:00  1534098445698535427                2   \n","5  2022-08-15 04:29:20+00:00  1504064898388606979               31   \n","6  2022-08-15 04:29:41+00:00  1442541458800594949               12   \n","7  2022-08-15 04:30:16+00:00           2398893018              426   \n","8  2022-08-15 04:30:20+00:00  1199768132832776193            32336   \n","9  2022-08-15 04:30:25+00:00            560323997             4371   \n","\n","   friends_count  favourites_count  retweet_count  \n","0          13797             33553              2  \n","1          13797             33553              2  \n","2          13797             33553              2  \n","3           3342             26845             16  \n","4             14                35              6  \n","5             18              2467            236  \n","6            161               236             96  \n","7            231                12              0  \n","8          26923             29239              1  \n","9           4999            234410           2938  \n","grammar\n","clean_text\n","grammar\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/tweet-topic-21-multi\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"arts_&_culture\",\n","    \"1\": \"business_&_entrepreneurs\",\n","    \"2\": \"celebrity_&_pop_culture\",\n","    \"3\": \"diaries_&_daily_life\",\n","    \"4\": \"family\",\n","    \"5\": \"fashion_&_style\",\n","    \"6\": \"film_tv_&_video\",\n","    \"7\": \"fitness_&_health\",\n","    \"8\": \"food_&_dining\",\n","    \"9\": \"gaming\",\n","    \"10\": \"learning_&_educational\",\n","    \"11\": \"music\",\n","    \"12\": \"news_&_social_concern\",\n","    \"13\": \"other_hobbies\",\n","    \"14\": \"relationships\",\n","    \"15\": \"science_&_technology\",\n","    \"16\": \"sports\",\n","    \"17\": \"travel_&_adventure\",\n","    \"18\": \"youth_&_student_life\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"arts_&_culture\": 0,\n","    \"business_&_entrepreneurs\": 1,\n","    \"celebrity_&_pop_culture\": 2,\n","    \"diaries_&_daily_life\": 3,\n","    \"family\": 4,\n","    \"fashion_&_style\": 5,\n","    \"film_tv_&_video\": 6,\n","    \"fitness_&_health\": 7,\n","    \"food_&_dining\": 8,\n","    \"gaming\": 9,\n","    \"learning_&_educational\": 10,\n","    \"music\": 11,\n","    \"news_&_social_concern\": 12,\n","    \"other_hobbies\": 13,\n","    \"relationships\": 14,\n","    \"science_&_technology\": 15,\n","    \"sports\": 16,\n","    \"travel_&_adventure\": 17,\n","    \"youth_&_student_life\": 18\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cardiffnlp--tweet-topic-21-multi/snapshots/e1a333ec0c1c59240f93df3403be60008a172933/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"pysentimiento/bertweet-hate-speech\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"hateful\",\n","    \"1\": \"targeted\",\n","    \"2\": \"aggressive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"aggressive\": 2,\n","    \"hateful\": 0,\n","    \"targeted\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["loaded topic model\n","0       [input_ids, attention_mask]\n","1       [input_ids, attention_mask]\n","2       [input_ids, attention_mask]\n","3       [input_ids, attention_mask]\n","4       [input_ids, attention_mask]\n","                   ...             \n","4995    [input_ids, attention_mask]\n","4996    [input_ids, attention_mask]\n","4997    [input_ids, attention_mask]\n","4998    [input_ids, attention_mask]\n","4999    [input_ids, attention_mask]\n","Name: clean_text, Length: 5000, dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/bertweet-hate-speech.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--pysentimiento--bertweet-hate-speech/snapshots/8913cd6a2515f3e033c3b097f68d3bfb41079c54/tokenizer_config.json\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n"]},{"output_type":"stream","name":"stdout","text":["predicted hate of tweets\n","loaded hate model\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"others\",\n","    \"1\": \"joy\",\n","    \"2\": \"sadness\",\n","    \"3\": \"anger\",\n","    \"4\": \"surprise\",\n","    \"5\": \"disgust\",\n","    \"6\": \"fear\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"anger\": 3,\n","    \"disgust\": 5,\n","    \"fear\": 6,\n","    \"joy\": 1,\n","    \"others\": 0,\n","    \"sadness\": 2,\n","    \"surprise\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"output_type":"stream","name":"stdout","text":["predicted emotion of tweets\n","loaded emotion model\n","got sentiment\n","got topic\n","got hate \n","got emo\n","got readability scores \n","now averaging users feeds\n","finshed averaging users feed \n"]}]},{"cell_type":"code","source":["df['user_id'][22]"],"metadata":{"id":"NhAJaJzxyyEY","colab":{"base_uri":"https://localhost:8080/","height":502},"executionInfo":{"status":"error","timestamp":1673804831788,"user_tz":0,"elapsed":485,"user":{"displayName":"Alvina","userId":"13758288938898952425"}},"outputId":"bbee6a89-48fd-48ba-e08c-9f3b46619fc1"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 22","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-969695c4ceb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 22"]}]},{"cell_type":"code","source":[],"metadata":{"id":"CvY7IH3Qo4ha"},"execution_count":null,"outputs":[]}]}